{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './wikihow_japanese/data/output/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n",
      "tgt\n",
      "title\n"
     ]
    }
   ],
   "source": [
    "print(test_data['src'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['脅威', 'を', '感じ', 'た', '蛇', 'は', '再び', '襲いかかり', 'ます']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger (\"-Owakati\")\n",
    "text = mecab.parse (\"脅威を感じた蛇は再び襲いかかります\").split()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rouge1(out,ref):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    out_list = mecab.parse(out).split()\n",
    "    ref_list = mecab.parse(ref).split()\n",
    "    mach = 0\n",
    "    for word in ref_list:\n",
    "        if word in out_list:\n",
    "            mach += 1\n",
    "    R = mach/len(ref_list)\n",
    "    P = mach/len(out_list)\n",
    "    Fm = 2*R*P/(R+P)\n",
    "    return(Fm)\n",
    "\n",
    "def Rouge2(out,ref):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    out_list = mecab.parse(out).split()\n",
    "    ref_list = mecab.parse(ref).split()\n",
    "    mach = 0\n",
    "    for i, word in enumerate(ref_list):\n",
    "        for j,word2 in enumerate(out_list[:-2]):\n",
    "            if word == word2 and ref_list[i+1]==out_list[j+1]:\n",
    "                mach += 1\n",
    "                \n",
    "    R = mach/(len(ref_list)-1)\n",
    "    P = mach/(len(out_list)-1)\n",
    "    Fm = 2*R*P/(R+P)\n",
    "    return(Fm)  \n",
    "\n",
    "def RougeL(out,ref):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    out_list = mecab.parse(out).split()\n",
    "    ref_list = mecab.parse(ref).split()\n",
    "    mach = 0\n",
    "    max_mach = 0\n",
    "    for i, word in enumerate(ref_list):\n",
    "        for j,word2 in enumerate(out_list[:-2]):\n",
    "            if word == word2:\n",
    "                l = 1\n",
    "                while j+l < len(out_list) and i+l < len(ref_list):\n",
    "                    if ref_list[i+l] == out_list[j+l]:\n",
    "                        l += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if l > max_mach:\n",
    "                    max_mach = l  \n",
    "  \n",
    "                \n",
    "    R = max_mach/(len(ref_list))\n",
    "    P = max_mach/(len(out_list))\n",
    "    Fm = 2*R*P/(R+P)\n",
    "    return(Fm)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "text1 = 'ライブがあるのは今日だ'\n",
    "text2 = '今日は楽しみにしていたライブがある'\n",
    "print(Rouge1(text1,text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2727272727272727\n",
      "0.42857142857142855\n",
      "0.33333333333333326\n"
     ]
    }
   ],
   "source": [
    "b = RougeL(text1,text2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'ja_ginza' (3.1.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "# コサイン類似度の計算\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def increment_edge (graph, node0, node1):\n",
    "    \n",
    "    if graph.has_edge(node0, node1):\n",
    "        graph[node0][node1][\"weight\"] += 1.0\n",
    "    else:\n",
    "        graph.add_edge(node0, node1, weight=1.0)\n",
    "        \n",
    "#USE文ごと\n",
    "def USE_sent(text):\n",
    "    doc = nlp(text)\n",
    "    sent_list = []\n",
    "    sent_vectors = []\n",
    "    summary = ''\n",
    "    graph = nx.Graph()\n",
    "    node_id = 0\n",
    "    for sent in doc.sents:\n",
    "        if sent.text not in sent_list:\n",
    "            sent_list.append(sent.text)\n",
    "            graph.add_node(node_id)\n",
    "            vector = np.ravel(np.array(use(sent.text)))\n",
    "            sent_vectors.append(vector)\n",
    "            node_id += 1\n",
    "    for i, vector1 in enumerate(sent_vectors[:-2]):\n",
    "        for l,vector2 in enumerate(sent_vectors[i+1:],i+1):\n",
    "            if cos_sim(vector1.T,vector2.T) > 0.4:\n",
    "                increment_edge(graph, i, l)\n",
    "    ranks = nx.pagerank(graph)\n",
    "    for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True)[:2]:\n",
    "        summary = summary+sent_list[node_id]\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error\n",
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
    "\n",
    "def link_sentence (doc, sent, lemma_graph, seen_lemma):\n",
    "    visited_tokens = []\n",
    "    visited_nodes = []\n",
    "\n",
    "    for i in range(sent.start, sent.end):\n",
    "        token = doc[i]\n",
    "\n",
    "        if token.pos_ in POS_KEPT and token.lemma_ not in slothlib_stopwords:\n",
    "            #token.lemma_は原型. token.pos_は品詞\n",
    "            key = (token.lemma_, token.pos_)\n",
    "\n",
    "            if key not in seen_lemma:\n",
    "                seen_lemma[key] = set([token.i])\n",
    "            else:\n",
    "                seen_lemma[key].add(token.i)\n",
    "\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "\n",
    "            if not node_id in lemma_graph:\n",
    "                lemma_graph.add_node(node_id)\n",
    "\n",
    "            #print(\"visit {} {}\".format(visited_tokens, visited_nodes))\n",
    "           # print(\"range {}\".format(list(range(len(visited_tokens) - 1, -1, -1))))\n",
    "            \n",
    "            for prev_token in range(len(visited_tokens) - 1, -1, -1):\n",
    "                #print(\"prev_tok {} {}\".format(prev_token, (token.i - visited_tokens[prev_token])))\n",
    "                \n",
    "                if (token.i - visited_tokens[prev_token]) <= 3:\n",
    "                    increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            #print(\" -- {} {} {} {} {} {}\".format(token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes))\n",
    "\n",
    "            visited_tokens.append(token.i)\n",
    "            visited_nodes.append(node_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def collect_phrases (chunk, phrases, counts):\n",
    "    chunk_len = chunk.end - chunk.start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk.start, chunk.end):\n",
    "        token = doc[i]\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        \n",
    "        if key in seen_lemma:\n",
    "            print(1)\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "            rank = ranks[node_id]\n",
    "            sq_sum_rank += rank\n",
    "            compound_key.add(key)\n",
    "        \n",
    "           # print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "        else:\n",
    "            non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.text.lower().replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "   # print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk.text, chunk.start, chunk.end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_5w1h import parse_5w1h\n",
    "import operator\n",
    "\n",
    "\n",
    "def pyrank_5w1h(text):\n",
    "    parse = parse_5w1h(0)\n",
    "    parse.extract(text)\n",
    "    doc = parse.doc\n",
    "    \n",
    "    lemma_graph = nx.Graph()\n",
    "    seen_lemma = {}\n",
    "    for sent in doc.sents:\n",
    "        link_sentence(doc, sent, lemma_graph, seen_lemma)\n",
    "        #break # only test one sentence\n",
    "    print(seen_lemma)\n",
    "    labels = {}\n",
    "    keys = list(seen_lemma.keys())\n",
    "\n",
    "    for i in range(len(seen_lemma)):\n",
    "        labels[i] = keys[i][0].lower()\n",
    "    ranks = nx.pagerank(lemma_graph)\n",
    "    \n",
    "    imp_list = []\n",
    "    for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
    "        imp_list.append(labels[node_id])\n",
    "        \n",
    "    phrases = {}\n",
    "    counts = {}\n",
    "    for sent in doc.sents:\n",
    "        collect_phrases(sent, phrases, counts)\n",
    "    print(phrases)\n",
    "    min_phrases = {}\n",
    "    \n",
    "    for compound_key, rank_tuples in phrases.items():\n",
    "        l = list(rank_tuples)\n",
    "        l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        phrase, rank = l[0]\n",
    "        count = counts[compound_key]\n",
    "        \n",
    "        min_phrases[phrase] = (rank, count)\n",
    "        print(min_phrases)\n",
    "    ans = ''\n",
    "    for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)[0:3]:\n",
    "            ans = ans+ phrase\n",
    "    return(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADP'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse = parse_5w1h(0)\n",
    "parse.extract(test_data['src'][0])\n",
    "doc = parse.doc\n",
    "doc[1].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('脅威', 'NOUN'): {0}, ('感ずる', 'VERB'): {2}, ('蛇', 'NOUN'): {146, 4, 20}, ('襲い掛かる', 'VERB'): {7}, ('従う', 'VERB'): {257, 10}, ('噛む', 'VERB'): {193, 13, 262}, ('速やか', 'ADJ'): {18, 39}, ('攻撃', 'NOUN'): {22}, ('範囲', 'NOUN'): {23}, ('離れる', 'VERB'): {25}, ('少ない', 'ADJ'): {28}, ('m', 'NOUN'): {31}, ('間合い', 'NOUN'): {33}, ('取る', 'VERB'): {35}, ('出来る限り', 'NOUN'): {38}, ('医療', 'NOUN'): {41}, ('処置', 'NOUN'): {42}, ('求める', 'VERB'): {121, 44, 311}, ('大切', 'ADJ'): {280, 47}, ('殆ど', 'NOUN'): {50}, ('病院', 'NOUN'): {161, 131, 69, 52, 185}, ('毒蛇', 'NOUN'): {260, 55}, ('抗毒素', 'NOUN'): {58}, ('血清', 'NOUN'): {60}, ('用意', 'VERB'): {63}, ('到着', 'VERB'): {71}, ('応急', 'NOUN'): {75}, ('手当て', 'NOUN'): {76}, ('症状', 'NOUN'): {82, 175}, ('改善', 'NOUN'): {84}, ('繋ぐ', 'VERB'): {87}, ('被害', 'NOUN'): {91}, ('現場', 'NOUN'): {92}, ('救急', 'NOUN'): {95}, ('サービス', 'NOUN'): {96}, ('通報', 'VERB'): {98}, ('出来る', 'VERB'): {99, 190, 227}, ('不幸', 'NOUN'): {101}, ('幸い', 'ADJ'): {104}, ('救急車', 'NOUN'): {107}, ('呼ぶ', 'VERB'): {109}, ('助け', 'NOUN'): {309, 119}, ('皆', 'NOUN'): {285, 123, 141}, ('被害者', 'NOUN'): {229, 127}, ('最寄り', 'NOUN'): {129}, ('搬送', 'VERB'): {133}, ('成る', 'VERB'): {137}, ('噛み付く', 'VERB'): {144}, ('ガラガラヘビ', 'NOUN'): {148}, ('分かる', 'VERB'): {153}, ('直行', 'VERB'): {163}, ('実際', 'NOUN'): {167}, ('毒', 'NOUN'): {169, 252, 206}, ('回る', 'VERB'): {173, 254}, ('出る', 'VERB'): {177}, ('始める', 'VERB'): {178}, ('居る', 'VERB'): {187, 292}, ('安心', 'VERB'): {189}, ('心臓', 'NOUN'): {198, 215}, ('置く', 'VERB'): {203}, ('含む', 'VERB'): {208}, ('血液', 'NOUN'): {210}, ('猛', 'NOUN'): {212}, ('スピード', 'NOUN'): {213}, ('流れ込む', 'VERB'): {217}, ('救助', 'NOUN'): {220}, ('来る', 'VERB'): {222}, ('静止', 'VERB'): {233}, ('動かす', 'VERB'): {240}, ('血流', 'NOUN'): {242}, ('増大', 'VERB'): {244}, ('言う', 'VERB'): {249}, ('動き', 'NOUN'): {269}, ('最小限', 'NOUN'): {271}, ('押さえる', 'VERB'): {273}, ('安静', 'ADJ'): {275}, ('周り', 'NOUN'): {288}}\n",
      "{(): {('できる限り速やかに医療処置を求めることが大切です。', 0.0), ('被害現場からすぐさま救急サービスに通報できれば不幸中の幸いです。', 0.0), ('病院に到着する前の応急手当だけでは、あまり症状の改善にはつながりません。', 0.0), ('救急車を呼べない場合は、何としても助けを求め、みなさんまたは被害者を最寄りの病院へ搬送しなければなりません。', 0.0), ('したがって、噛まれた際は速やかに蛇の攻撃範囲から離れましょう。', 0.0), ('脅威を感じた蛇は再び襲いかかります。', 0.0), ('したがって、毒蛇に噛まれた際は体の動きを最小限に抑えて安静にすることが大切です。', 0.0), ('みなさんに噛みついた蛇がガラガラヘビかどうかが分からない場合でも、すぐに病院へ直行しましょう。', 0.0), ('少なくとも6mは間合いを取りましょう。', 0.0), ('実際に毒が体に回り、症状が出始めたとしても、病院にいれば安心できるでしょう。', 0.0), ('体を動かすと血流が増大し、あっという間に毒が回ります。', 0.0), ('もちろん、みなさんの周りに誰もいなければ、じっとしている場合ではありません。', 0.0), ('噛まれた箇所を心臓よりも上に置くと、毒を含んだ血液が猛スピードで心臓に流れ込みます。', 0.0), ('救助が来るまでの間、できれば被害者の体を静止させましょう。', 0.0), ('ほとんどの病院は、毒蛇用の抗毒素（血清）を用意しています。', 0.0), ('すぐに助けを求めましょう。', 0.0)}}\n",
      "{'できる限り速やかに医療処置を求めることが大切です。': (0.0, 16)}\n",
      "できる限り速やかに医療処置を求めることが大切です。\n",
      "したがって、毒蛇に噛まれた際は体の動きを最小限に抑えて安静にすることが大切です。救急車を呼べない場合は、何としても助けを求め、みなさんまたは被害者を最寄りの病院へ搬送しなければなりません。\n"
     ]
    }
   ],
   "source": [
    "print(pyrank_5w1h(test_data['src'][0]))\n",
    "print(USE_sent(test_data['src'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38414129782617307\n"
     ]
    }
   ],
   "source": [
    "score1 = 0\n",
    "mean = 0\n",
    "for i,test in enumerate(test_data['src'][:10]):\n",
    "    score1 = score1 +  Rouge1(test_data['tgt'][i],USE_sent(test))\n",
    "    \n",
    "print(score1/len(test_data['src'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
