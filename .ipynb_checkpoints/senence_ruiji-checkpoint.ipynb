{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金曜日の朝、ケンタッキー州の工業団地で巨大な火事が発生し、当局が被害を封じ込めようと働いたため、その地域に濃い煙が噴出した。市の緊急管理機関のマイク・ワイマー氏によると、ルイビルのゼネラル・エレクトリック・アプライアンス・パークで午前7時少し前に火災が始まりました。 彼は、負傷者や閉じ込められた者の報告はないと言った。 ビデオは煙と明るいオレンジ色の炎の両方を示しました。 消防士は影響を受けた建物の周りの位置を取り、周辺から水を噴霧しました。 ワイマーは、当局が火災の原因を知らなかったとCNNに語り、少なくとも4つの警報が鳴った。 GEのWebサイトによると、ルイビルアプライアンスパークの施設は、米国の製造業を活性化しています。 公園は大きく、34のサッカー場が施設内の倉庫の1つに収まるようになっています。\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.error\n",
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "from parse_5w1h import parse_5w1h\n",
    "#テストファイル読み込み&下処理\n",
    "path = \"/Users/shota/Documents/itac/test_summary1.txt\"\n",
    "with open(path) as f:\n",
    "    text = f.read()\n",
    "text = '金曜日の朝、ケンタッキー州の工業団地で巨大な火事が発生し、当局が被害を封じ込めようと働いたため、その地域に濃い煙が噴出した。市の緊急管理機関のマイク・ワイマー氏によると、ルイビルのゼネラル・エレクトリック・アプライアンス・パークで午前7時少し前に火災が始まりました。 彼は、負傷者や閉じ込められた者の報告はないと言った。 ビデオは煙と明るいオレンジ色の炎の両方を示しました。 消防士は影響を受けた建物の周りの位置を取り、周辺から水を噴霧しました。 ワイマーは、当局が火災の原因を知らなかったとCNNに語り、少なくとも4つの警報が鳴った。 GEのWebサイトによると、ルイビルアプライアンスパークの施設は、米国の製造業を活性化しています。 公園は大きく、34のサッカー場が施設内の倉庫の1つに収まるようになっています。'\n",
    "text = text.replace('\\n','')\n",
    "\n",
    "print(text)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙サイズ: 109\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. 単語の頻度をカウント\n",
    "word2freq = defaultdict(int)\n",
    "word_list = []\n",
    "for sent in doc.sents:\n",
    "    word_list = [word.text for word in sent] \n",
    "    for word in set(word_list) :\n",
    "            word2freq[word] += 1\n",
    "print(\"語彙サイズ: %d\" % len(word2freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙サイズ: 109\n"
     ]
    }
   ],
   "source": [
    "# 2. 語彙制限（高頻度な500単語に制限）\n",
    "\n",
    "vocab = list()\n",
    "for word, freq in sorted(word2freq.items(), key=lambda x: x[1], reverse=True)[:500]:\n",
    "    vocab.append(word)\n",
    "print(\"語彙サイズ: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語 ケンタッキー州のID: 27\n"
     ]
    }
   ],
   "source": [
    "# 3. 各単語にIDを割り当てる\n",
    "\n",
    "word2id = dict()\n",
    "for word in vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "print(\"単語 ケンタッキー州のID: %s\" % word2id[\"ケンタッキー州\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TFIDFの計算\n",
    "def tfidf(word, word_list, word2freq, n):\n",
    "    return (word_list.count(word) / len(word_list)) * math.log(n / word2freq[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BoWの重み付け\n",
    "import math\n",
    "import numpy as np\n",
    "def tfidf_vectorize(sent, vocab, word2id, word2df, n):\n",
    "    vectors = list()\n",
    "    \n",
    "    word_list = [token.text for token in sent ]\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for word in word_list:\n",
    "        if word in vocab:\n",
    "            vector[word2id[word]] = tfidf(word, word_list, word2df, n)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phrase_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5e787a011f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mphrase_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phrase_list' is not defined"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "sent_list = []\n",
    "sent_vectors = []\n",
    "graph = nx.Graph()\n",
    "\n",
    "node_id = 0\n",
    "for sent in doc.sents:\n",
    "    if sent.text not in sent_list:\n",
    "        sent_list.append(sent.text)\n",
    "        graph.add_node(node_id)\n",
    "        vector = tfidf_vectorize(sent,vocab,word2id,word2freq, len([1 for token in sent])) \n",
    "        sent_vectors.append(vector)\n",
    "        node_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コサイン類似度の計算\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_edge (graph, node0, node1):\n",
    "    print(\"link {} {}\".format(node0, node1))\n",
    "    \n",
    "    if graph.has_edge(node0, node1):\n",
    "        graph[node0][node1][\"weight\"] += 1.0\n",
    "    else:\n",
    "        graph.add_edge(node0, node1, weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度が0以上の場合edgeで繋ぐ\n",
    "for i, vector1 in enumerate(sent_vectors[:-2]):\n",
    "    for l,vector2 in enumerate(sent_vectors[i+1:],i+1):\n",
    "        if cos_sim(vector1,vector2) > 0.2:\n",
    "            increment_edge(graph, i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "pos = nx.spring_layout(graph)\n",
    "\n",
    "nx.draw(graph, pos=pos, with_labels=False, font_weight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = nx.pagerank(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(rank, phrase_list[node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#金曜日の朝、ケンタッキー州の工業団地で巨大な火事が発生し、当局が被害を封じ込めようと働いたため、その地域に濃い煙が噴出した。\n",
    "#彼は、負傷者や閉じ込められた者の報告はないと言った。\n",
    "#消防士は影響を受けた建物の周りの位置を取り、周辺から水を噴霧しました。\n",
    "#ワイマーは、当局が火災の原因を知らなかったとCNNに語り、少なくとも4つの警報が鳴った。\n",
    "#公園は大きく、34のサッカー場が施設内の倉庫の1つに収まるようになっています。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
