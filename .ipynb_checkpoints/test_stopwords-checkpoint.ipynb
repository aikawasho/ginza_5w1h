{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ginza\n",
    "stopwords = list(ginza.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error\n",
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slothlib_stopwords)\n",
    "print('こんにちは' not in slothlib_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "from parse_5w1h import parse_5w1h\n",
    "#テストファイル読み込み&下処理\n",
    "path = \"/Users/shota/Documents/itac/test_summary1.txt\"\n",
    "with open(path) as f:\n",
    "    text = f.read()\n",
    "text = text.replace('\\n','')\n",
    "print(text)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def increment_edge (graph, node0, node1):\n",
    "    print(\"link {} {}\".format(node0, node1))\n",
    "    \n",
    "    if graph.has_edge(node0, node1):\n",
    "        graph[node0][node1][\"weight\"] += 1.0\n",
    "    else:\n",
    "        graph.add_edge(node0, node1, weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
    "\n",
    "def link_sentence (doc, sent, lemma_graph, seen_lemma):\n",
    "    visited_tokens = []\n",
    "    visited_nodes = []\n",
    "\n",
    "    for i in range(sent.start, sent.end):\n",
    "        token = doc[i]\n",
    "\n",
    "        if token.pos_ in POS_KEPT and token.lemma_ not in slothlib_stopwords:\n",
    "            #token.lemma_は原型. token.pos_は品詞\n",
    "            key = (token.lemma_, token.pos_)\n",
    "\n",
    "            if key not in seen_lemma:\n",
    "                seen_lemma[key] = set([token.i])\n",
    "            else:\n",
    "                seen_lemma[key].add(token.i)\n",
    "\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "\n",
    "            if not node_id in lemma_graph:\n",
    "                lemma_graph.add_node(node_id)\n",
    "\n",
    "            print(\"visit {} {}\".format(visited_tokens, visited_nodes))\n",
    "            print(\"range {}\".format(list(range(len(visited_tokens) - 1, -1, -1))))\n",
    "            \n",
    "            for prev_token in range(len(visited_tokens) - 1, -1, -1):\n",
    "                print(\"prev_tok {} {}\".format(prev_token, (token.i - visited_tokens[prev_token])))\n",
    "                \n",
    "                if (token.i - visited_tokens[prev_token]) <= 3:\n",
    "                    increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            print(\" -- {} {} {} {} {} {}\".format(token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes))\n",
    "\n",
    "            visited_tokens.append(token.i)\n",
    "            visited_nodes.append(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_graph = nx.Graph()\n",
    "seen_lemma = {}\n",
    "\n",
    "for sent in doc.sents:\n",
    "    link_sentence(doc, sent, lemma_graph, seen_lemma)\n",
    "    #break # only test one sentence\n",
    "labels = {}\n",
    "keys = list(seen_lemma.keys())\n",
    "\n",
    "for i in range(len(seen_lemma)):\n",
    "    labels[i] = keys[i][0].lower()\n",
    "ranks = nx.pagerank(lemma_graph)\n",
    "imp_list = []\n",
    "for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
    "    imp_list.append(labels[node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def collect_5w1hphrases (chunk,chunk_start,chunk_end, phrases, counts):\n",
    "    chunk_len = chunk_end - chunk_start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk_start, chunk_end):\n",
    "        token = doc[i]\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        \n",
    "        if key in seen_lemma:\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "            rank = ranks[node_id]\n",
    "            sq_sum_rank += rank\n",
    "            compound_key.add(key)\n",
    "        \n",
    "            #print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "        else:\n",
    "            non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "    #print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk, chunk_start, chunk_end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_5w1h import parse_5w1h\n",
    "parse = parse_5w1h(0)\n",
    "parse.extract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parse.display_5w1h()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phrases = {}\n",
    "counts = {}\n",
    "\n",
    "for i,chunk in enumerate(parse._5w1hs):\n",
    "\n",
    "     collect_5w1hphrases(chunk, parse._5w1h_s[i],parse._5w1h_e[i],phrases, counts)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "min_phrases = {}\n",
    "\n",
    "for compound_key, rank_tuples in phrases.items():\n",
    "    l = list(rank_tuples)\n",
    "    l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    phrase, rank = l[0]\n",
    "    count = counts[compound_key]\n",
    "    \n",
    "    min_phrases[phrase] = (rank, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_phrases[phrase][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for phrase, (rank, count) in min_phrases.items():\n",
    "\n",
    "    if rank > 0.05:\n",
    "        #summary.append(phrase)\n",
    "        if parse.doc[parse._5w1h_e[i]]._._5w1h == 'How':\n",
    "            l = i\n",
    "            while l > 0:\n",
    "                print(parse._5w1hs[l-1])\n",
    "                summary.append(parse._5w1hs[l-1])\n",
    "                if parse.doc[parse._5w1h_e[l-1]]._._5w1h == 'Who'or parse.doc[parse._5w1h_e[l-1]]._._5w1h == 'What':\n",
    "                    break\n",
    "                l=l-1\n",
    "        else:\n",
    "            l = i\n",
    "            while l < len(parse._5w1hs)-1:\n",
    "                summary.append(parse._5w1hs[l+1])\n",
    "                if parse.doc[parse._5w1h_e[l+1]]._._5w1h == 'How':\n",
    "                    break\n",
    "                l=l+1\n",
    "                \n",
    "        print(''.join(summary))\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunk in enumerate(parse._5w1hs):\n",
    "    summary = []\n",
    "    rank = collect_5w1hphrases(chunk, parse._5w1h_s[i],parse._5w1h_e[i],phrases, counts)\n",
    "    #print(parse.doc[parse._5w1h_e[i]-1]._._5w1h)\n",
    "    if rank > 0.05:\n",
    "        summary.append(chunk)\n",
    "        print(chunk)\n",
    "        print(parse.doc[parse._5w1h_e[i]]._._5w1h)\n",
    "        if parse.doc[parse._5w1h_e[i]]._._5w1h == 'How':\n",
    "            l = i\n",
    "            while l > 0:\n",
    "                \n",
    "                summary.append(parse._5w1hs[l-1])\n",
    "                if parse.doc[parse._5w1h_e[l-1]]._._5w1h == 'Who'or parse.doc[parse._5w1h_e[l-1]]._._5w1h == 'What':\n",
    "                    break\n",
    "                l=l-1\n",
    "        else:\n",
    "            l = i\n",
    "            while l < len(parse._5w1hs)-1:\n",
    "                summary.append(parse._5w1hs[l+1])\n",
    "                if parse.doc[parse._5w1h_e[l+1]]._._5w1h == 'How':\n",
    "                    break\n",
    "                l=l+1\n",
    "                \n",
    "        #print(''.join(summary))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)[0:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase, (rank, count) in min_phrases.items():\n",
    "    print(phrase)\n",
    "    print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(min_phrases.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_phrases.items()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def collect_phrases (chunk, phrases, counts):\n",
    "    chunk_len = chunk.end - chunk.start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk.start, chunk.end):\n",
    "        token = doc[i]\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        \n",
    "        if key in seen_lemma:\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "            rank = ranks[node_id]\n",
    "            sq_sum_rank += rank\n",
    "            compound_key.add(key)\n",
    "        \n",
    "            print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "        else:\n",
    "            non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.text.lower().replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "    print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk.text, chunk.start, chunk.end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = {}\n",
    "counts = {}\n",
    "for sent in doc.sents:\n",
    "    collect_phrases(sent, phrases, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "min_phrases = {}\n",
    "\n",
    "for compound_key, rank_tuples in phrases.items():\n",
    "    l = list(rank_tuples)\n",
    "    l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    phrase, rank = l[0]\n",
    "    count = counts[compound_key]\n",
    "    \n",
    "    min_phrases[phrase] = (rank, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)[0:3]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
