{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './wikihow_japanese/data/output/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_json(path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                     src  \\\n",
      "0     脅威を感じた蛇は再び襲いかかります。したがって、噛まれた際は速やかに蛇の攻撃範囲から離れまし...   \n",
      "1     噛み傷の周囲は大きく腫れ上がります。傷口の周りの衣類はすべて取り除きましょう。また、患部に付...   \n",
      "2     被害者に話しかけましょう。次々に質問を投げかけ、できる限り被害者の意識を噛み傷から逸らしまし...   \n",
      "3      世間によくある誤解の一つが、噛み傷を切り取れば毒を取り除けるというものです。しかし実際のと...   \n",
      "4     秋が近づいて夜が涼しくなってきたら、翌年の春に向けて球根を植えましょう。初霜が降りると、土が...   \n",
      "...                                                 ...   \n",
      "1019  正しい方法で洗顔をすると、汚れや皮脂を取り除けるだけではなく、メイクも落すことができます。必...   \n",
      "1020  厚塗りになるのを避けるため、ファンデーションは少量から塗り始め、必要があれば足すのが理想的で...   \n",
      "1021  ファンデーションを塗る前に、自分の肌質を知ることが大切です。肌質は、乾燥肌、皮脂肌、普通肌ま...   \n",
      "1022  ガス漏れが発生すると、周辺が汚染され危険な状態になります。自動車のエンジンをかけたり、芝刈り...   \n",
      "1023  弁を時計方向に回して締めましょう。ガス漏れの防止につながります。腐った卵のような臭いがする場...   \n",
      "\n",
      "                                                    tgt  \\\n",
      "0     ガラガラヘビから離れましょう。医療処置を受けましょう。決して患部を心臓よりも高い位置に置いて...   \n",
      "1     衣類やアクセサリーを外しましょう。傷口の出血はしばらくそのままにしましょう。吸引ポンプを使い...   \n",
      "2     被害者を落ち着かせましょう。患部の腫れや変色に注意しましょう。ショック症状のサインに注意しま...   \n",
      "3     傷口を切除してはいけません。口で毒を吸い出してはいけません。止血帯で患部を縛ってはいけません...   \n",
      "4                     植え付けは秋に行う。球根を選ぶ。チューリップを植える場所を決める。   \n",
      "...                                                 ...   \n",
      "1019  顔を洗う。角質を落とし肌を整える。保湿剤を塗る。プライマーを使う。カラーコレクターで肌色を補...   \n",
      "1020  少量から塗り始める。外側に向かって伸ばす。なじませる。気になる部分を隠す。セッティングパウダ...   \n",
      "1021              肌質に合わせて選ぶ。肌色に合ったファンデーションを選ぶ。カバー力を決める。   \n",
      "1022  屋内や物置に置かない。乾燥して換気の良い屋外の場所に置く。冬の時期も温度が零下40度を下回ら...   \n",
      "1023  使用していない時は弁が閉じられていることを確認する。ラベルを剥がし、錆ついていないか確認する...   \n",
      "\n",
      "                        title  \n",
      "0     How_toガラガラヘビに噛まれた時の対処_0  \n",
      "1     How_toガラガラヘビに噛まれた時の対処_1  \n",
      "2     How_toガラガラヘビに噛まれた時の対処_2  \n",
      "3     How_toガラガラヘビに噛まれた時の対処_3  \n",
      "4          How_toチューリップを植える_0  \n",
      "...                       ...  \n",
      "1019      How_toファンデーションを塗る_0  \n",
      "1020      How_toファンデーションを塗る_1  \n",
      "1021      How_toファンデーションを塗る_2  \n",
      "1022  How_to屋外でプロパンボンベを保管する_0  \n",
      "1023  How_to屋外でプロパンボンベを保管する_1  \n",
      "\n",
      "[1024 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(test_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['脅威', 'を', '感じ', 'た', '蛇', 'は', '再び', '襲いかかり', 'ます']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger (\"-Owakati\")\n",
    "text = mecab.parse (\"脅威を感じた蛇は再び襲いかかります\").split()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rouge1(out,ref):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    out_list = mecab.parse(out).split()\n",
    "    ref_list = mecab.parse(ref).split()\n",
    "    mach = 0\n",
    "    for word in ref_list:\n",
    "        if word in out_list:\n",
    "            mach += 1\n",
    "    R = mach/len(ref_list)\n",
    "    if len(out_list) != 0:\n",
    "        P = mach/(len(out_list))\n",
    "    else:\n",
    "        P = 0\n",
    "    Fm = 2*R*P/(R+P)\n",
    "    return(Fm)\n",
    "\n",
    "def Rouge2(out,ref):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    out_list = mecab.parse(out).split()\n",
    "    ref_list = mecab.parse(ref).split()\n",
    "    mach = 0\n",
    "    for i, word in enumerate(ref_list):\n",
    "        for j,word2 in enumerate(out_list[:-2]):\n",
    "            if word == word2 and ref_list[i+1]==out_list[j+1]:\n",
    "                mach += 1\n",
    "                \n",
    "    R = mach/(len(ref_list)-1)\n",
    "    if len(out_list) != 0:\n",
    "        P = mach/(len(out_list))\n",
    "    else:\n",
    "        P = 0\n",
    "    Fm = 2*R*P/(R+P)\n",
    "    return(Fm)  \n",
    "\n",
    "def RougeL(out,ref):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    out_list = mecab.parse(out).split()\n",
    "    ref_list = mecab.parse(ref).split()\n",
    "    mach = 0\n",
    "    max_mach = 0\n",
    "    for i, word in enumerate(ref_list):\n",
    "        for j,word2 in enumerate(out_list[:-2]):\n",
    "            if word == word2:\n",
    "                l = 1\n",
    "                while j+l < len(out_list) and i+l < len(ref_list):\n",
    "                    if ref_list[i+l] == out_list[j+l]:\n",
    "                        l += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if l > max_mach:\n",
    "                    max_mach = l  \n",
    "  \n",
    "                \n",
    "    R = max_mach/(len(ref_list))\n",
    "    if len(out_list) != 0:\n",
    "        P = max_mach/(len(out_list))\n",
    "    else:\n",
    "        P = 0\n",
    "    Fm = 2*R*P/(R+P)\n",
    "    return(Fm)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "text1 = 'ライブがあるのは今日だ'\n",
    "text2 = '今日は楽しみにしていたライブがある'\n",
    "print(Rouge1(text1,text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333326\n"
     ]
    }
   ],
   "source": [
    "b = RougeL(text1,text2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "# コサイン類似度の計算\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def increment_edge (graph, node0, node1):\n",
    "    \n",
    "    if graph.has_edge(node0, node1):\n",
    "        graph[node0][node1][\"weight\"] += 1.0\n",
    "    else:\n",
    "        graph.add_edge(node0, node1, weight=1.0)\n",
    "        \n",
    "#USE文ごと\n",
    "def USE_sent(text):\n",
    "    doc = nlp(text)\n",
    "    sent_list = []\n",
    "    sent_vectors = []\n",
    "    summary = ''\n",
    "    graph = nx.Graph()\n",
    "    node_id = 0\n",
    "    for sent in doc.sents:\n",
    "        if sent.text not in sent_list:\n",
    "            sent_list.append(sent.text)\n",
    "            graph.add_node(node_id)\n",
    "            vector = np.ravel(np.array(use(sent.text)))\n",
    "            sent_vectors.append(vector)\n",
    "            node_id += 1\n",
    "    for i, vector1 in enumerate(sent_vectors[:-2]):\n",
    "        for l,vector2 in enumerate(sent_vectors[i+1:],i+1):\n",
    "            if cos_sim(vector1.T,vector2.T) > 0.4:\n",
    "                increment_edge(graph, i, l)\n",
    "    ranks = nx.pagerank(graph)\n",
    "    for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True)[:2]:\n",
    "        summary = summary+sent_list[node_id]\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error\n",
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
    "\n",
    "def link_sentence (doc, sent, lemma_graph, seen_lemma):\n",
    "    visited_tokens = []\n",
    "    visited_nodes = []\n",
    "\n",
    "    for i in range(sent.start, sent.end):\n",
    "        token = doc[i]\n",
    "\n",
    "        if token.pos_ in POS_KEPT and token.lemma_ not in slothlib_stopwords:\n",
    "            #token.lemma_は原型. token.pos_は品詞\n",
    "            key = (token.lemma_, token.pos_)\n",
    "            \n",
    "            if key not in seen_lemma:\n",
    "                seen_lemma[key] = set([token.i])\n",
    "            else:\n",
    "                seen_lemma[key].add(token.i)\n",
    "\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "\n",
    "            if not node_id in lemma_graph:\n",
    "                lemma_graph.add_node(node_id)\n",
    "\n",
    "            #print(\"visit {} {}\".format(visited_tokens, visited_nodes))\n",
    "           # print(\"range {}\".format(list(range(len(visited_tokens) - 1, -1, -1))))\n",
    "            \n",
    "            for prev_token in range(len(visited_tokens) - 1, -1, -1):\n",
    "                #print(\"prev_tok {} {}\".format(prev_token, (token.i - visited_tokens[prev_token])))\n",
    "                \n",
    "                if (token.i - visited_tokens[prev_token]) <= 3:\n",
    "                    increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            #print(\" -- {} {} {} {} {} {}\".format(token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes))\n",
    "\n",
    "            visited_tokens.append(token.i)\n",
    "            visited_nodes.append(node_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def collect_phrases (chunk, phrases, counts,seen_lemma,ranks):\n",
    "    chunk_len = chunk.end - chunk.start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk.start, chunk.end):\n",
    "        if i < len(doc):\n",
    "            token = doc[i]\n",
    "            key = (token.lemma_, token.pos_)\n",
    "  \n",
    "            if key in seen_lemma:\n",
    "                node_id = list(seen_lemma.keys()).index(key)\n",
    "                rank = ranks[node_id]\n",
    "                sq_sum_rank += rank\n",
    "                compound_key.add(key)\n",
    "\n",
    "               # print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "            else:\n",
    "                non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.text.lower().replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "   # print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk.text, chunk.start, chunk.end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_5w1h import parse_5w1h\n",
    "import operator\n",
    "\n",
    "#pytextrank文ごと\n",
    "def pyrank_sent(text):\n",
    "    parse = parse_5w1h(0)\n",
    "    parse.extract(text)\n",
    "    doc = parse.doc\n",
    "    \n",
    "    lemma_graph = nx.Graph()\n",
    "    seen_lemma = {}\n",
    "    for sent in doc.sents:\n",
    "        link_sentence(doc, sent, lemma_graph, seen_lemma)\n",
    "        #break # only test one sentence\n",
    "    labels = {}\n",
    "    keys = list(seen_lemma.keys())\n",
    "\n",
    "    for i in range(len(seen_lemma)):\n",
    "        labels[i] = keys[i][0].lower()\n",
    "    ranks = nx.pagerank(lemma_graph)\n",
    "        \n",
    "    phrases = {}\n",
    "    counts = {}\n",
    "    for sent in doc.sents:\n",
    "        collect_phrases(sent, phrases, counts,seen_lemma,ranks)\n",
    "    min_phrases = {}\n",
    "    \n",
    "    for compound_key, rank_tuples in phrases.items():\n",
    "        l = list(rank_tuples)\n",
    "        l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        phrase, rank = l[0]\n",
    "        count = counts[compound_key]\n",
    "        \n",
    "        min_phrases[phrase] = (rank, count)\n",
    "    ans = ''\n",
    "    for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)[0:3]:\n",
    "            ans = ans+ phrase\n",
    "    return(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_5w1hphrases (chunk,chunk_start,chunk_end, phrases,counts,seen_lemma,ranks):\n",
    "    chunk_len = chunk_end - chunk_start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk_start, chunk_end):\n",
    "        if i < len(doc):\n",
    "            token = doc[i]\n",
    "            key = (token.lemma_, token.pos_)\n",
    "  \n",
    "            if key in seen_lemma:\n",
    "                node_id = list(seen_lemma.keys()).index(key)\n",
    "                rank = ranks[node_id]\n",
    "                sq_sum_rank += rank\n",
    "                compound_key.add(key)\n",
    "\n",
    "               # print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "            else:\n",
    "                non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "    #print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk, chunk_start, chunk_end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyrank_5w1h(text):\n",
    "    parse = parse_5w1h(0)\n",
    "    parse.extract(text)\n",
    "    _5w1h = parse.display_5w1h()\n",
    "    doc = parse.doc\n",
    "    \n",
    "    lemma_graph = nx.Graph()\n",
    "    seen_lemma = {}\n",
    "    for sent in doc.sents:\n",
    "        link_sentence(doc, sent, lemma_graph, seen_lemma)\n",
    "        #break # only test one sentence\n",
    "    labels = {}\n",
    "    keys = list(seen_lemma.keys())\n",
    "\n",
    "    for i in range(len(seen_lemma)):\n",
    "        labels[i] = keys[i][0].lower()\n",
    "    ranks = nx.pagerank(lemma_graph)\n",
    "    phrases = {}\n",
    "    counts = {}\n",
    "\n",
    "    for i,chunk in enumerate(_5w1h):\n",
    "\n",
    "         collect_5w1hphrases(chunk.phrase, chunk.start,chunk.end,phrases,counts,seen_lemma,ranks)\n",
    "    min_ranks = {}\n",
    "\n",
    "    for compound_key, rank_tuples in phrases.items():\n",
    "        l = list(rank_tuples)\n",
    "        l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        phrase, rank = l[0]\n",
    "\n",
    "        min_ranks[phrase] = (rank)\n",
    "    ans = ''\n",
    "    i = 0 \n",
    "    summary = []\n",
    "    count = []\n",
    "    threshhold = sum(ranks.values())/len(ranks.values())\n",
    "    while i < len(_5w1h):\n",
    "        if _5w1h[i].phrase in min_ranks:\n",
    "            if min_ranks[_5w1h[i].phrase] >threshhold:\n",
    "                if _5w1h[i].phrase not in count:\n",
    "                    summary.append(_5w1h[i].phrase)\n",
    "                    count.append(_5w1h[i].phrase)\n",
    "\n",
    "                if _5w1h[i]._type ==\"How\":\n",
    "                    l = i-1\n",
    "                    while l >= 0:\n",
    "                        if _5w1h[l].phrase not in count:\n",
    "                            summary.insert(0,_5w1h[l].phrase)\n",
    "                            count.append(_5w1h[l].phrase)\n",
    "                        if _5w1h[l]._type == 'Who'or _5w1h[l]._type == 'What':\n",
    "                            if summary != []:\n",
    "                                ans = ans + ''.join(summary)\n",
    "                            summary = []\n",
    "                            break\n",
    "                        l -= 1\n",
    "                else:\n",
    "                    i = i+1\n",
    "                    while i < len(_5w1h):\n",
    "                        if _5w1h[i].phrase not in count:\n",
    "                            summary.append(_5w1h[i].phrase)\n",
    "                            count.append(_5w1h[i].phrase)\n",
    "                        if _5w1h[i]._type == 'How' :\n",
    "                            if summary != []:\n",
    "                                ans = ans + ''.join(summary)\n",
    "                            summary = []\n",
    "                            break\n",
    "                        i += 1\n",
    "\n",
    "        i += 1\n",
    "    return(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "脅威を感じた蛇は再び襲いかかります。したがって、噛まれた際は速やかに蛇の攻撃範囲から離れましょう。少なくとも6mは間合いを取りましょう。できる限り速やかに医療処置を求めることが大切です。ほとんどの病院は、毒蛇用の抗毒素（血清）を用意しています。病院に到着する前の応急手当だけでは、あまり症状の改善にはつながりません。被害現場からすぐさま救急サービスに通報できれば不幸中の幸いです。救急車を呼べないみなさんまたは被害者を最寄りの病院へ搬送しなければなりません。みなさんに噛みついた蛇がガラガラヘビかどうかが分からない場合でも、すぐに病院へ直行しましょう。実際に毒が体に回り、症状が出始めたとしても、病院にいれば安心できるでしょう。噛まれた箇所を心臓よりも上に置くと、毒を含んだ血液が猛スピードで心臓に流れ込みます。救助が来るまでの間、できれば体を動かすと血流が増大し、という間に毒が回ります。毒蛇に噛まれた際は体の動きを最小限に抑えて安静にすることがもちろん、みなさんの周りに誰もいなければ、すぐに助けを求めましょう。\n"
     ]
    }
   ],
   "source": [
    "#print(pyrank_sent(test_data['src'][0]))\n",
    "print(pyrank_5w1h(test_data['src'][0]))\n",
    "#print(USE_sent(test_data['src'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE文ごと_ROUGE-1 : 0.5467435369356476\n",
      "pytextrank文ごと_ROUGE-1 : 0.49806015699450645\n"
     ]
    }
   ],
   "source": [
    "USE_s = 0\n",
    "pytext_s = 0\n",
    "pytext_5 = 0\n",
    "mean = 0\n",
    "for i,test in enumerate(test_data['src'][:100]):\n",
    "    USE_s = USE_s +  Rouge1(USE_sent(test),test_data['tgt'][i])\n",
    "    pytext_s = pytext_s + Rouge1(pyrank_sent(test),test_data['tgt'][i])\n",
    "    #pytext_5 = pytext_5 + Rouge1(pyrank_5w1h(test),test_data['tgt'][i])\n",
    "    \n",
    "print('USE文ごと_ROUGE-1 : {}'.format(USE_s/len(test_data['src'][:100])))\n",
    "print('pytextrank文ごと_ROUGE-1 : {}'.format(pytext_s/len(test_data['src'][:100])))\n",
    "#print('pytextrank5w1h_ROUGE-1 : {}'.format( pytext_5/len(test_data['src'][:100])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE文ごと_ROUGE-1 : 0.023184261502556206\n",
      "pytextrank文ごと_ROUGE-1 : 0.02451727743168565\n",
      "pytextrank5w1h_ROUGE-1 : 0.02003094912751514\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
