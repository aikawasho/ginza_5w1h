{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ginza\n",
    "stopwords = list(ginza.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['たり', 'ぶり', 'お', 'する', 'み', 'ため', 'る', 'なけれ', 'しか', 'あっ', 'いつ', 'つけ', 'ね', 'らしい', 'とも', 'できる', 'おけ', 'い', 'ひと', 'さらに', 'その', 'もと', 'たら', 'ます', 'あまり', 'おい', 'さん', 'でき', 'ば', 'くん', 'なる', 'くる', 'なら', 'あるいは', 'いい', 'および', 'おり', 'たち', 'これ', 'れる', 'とっ', 'ご', 'おら', 'や', 'よく', 'しまう', 'ない', 'かつ', 'ごと', 'この', 'か', 'いる', 'ら', 'れ', 'き', 'です', 'せ', 'の', 'いっ', 'ち', 'より', 'はじめ', 'ぬ', 'なく', 'とき', 'られる', 'よ', 'で', 'ん', 'なっ', 'たい', 'かけ', 'うち', 'ところ', 'と', 'こ', 'そこ', 'よっ', 'もっ', 'られ', 'なり', 'そう', 'もの', 'な', 'こう', 'だけ', 'また', 'なお', 'は', 'ほぼ', 'しかし', 'だ', 'いわ', 'つい', 'そして', 'に', 'も', 'やっ', 'のみ', 'にて', 'のち', 'それ', 'きっかけ', 'が', 'え', 'あれ', 'しよう', 'べき', 'さ', 'から', 'ほとんど', 'まま', 'あ', 'た', 'まで', 'つ', 'それぞれ', 'どう', 'など', 'かなり', 'つつ', 'しまっ', 'いずれ', 'せい', '一', 'もう', 'なかっ', 'ず', 'へ', 'す', 'ながら', 'よう', 'すぐ', 'ま', 'すべて', 'なし', 'ただし', 'ほど', 'かつて', 'せる', 'ここ', 'を', 'て', 'いう', 'し', 'ちゃん', 'ほか', 'あり', 'よれ', 'いく', 'だっ', 'よる', 'こと', 'ある']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error\n",
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ', 'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか', 'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら', 'きた', 'くせ', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', 'ごっちゃ', 'これ', 'これら', 'ごろ', 'さまざま', 'さらい', 'さん', 'しかた', 'しよう', 'すか', 'ずつ', 'すね', 'すべて', 'ぜんぶ', 'そう', 'そこ', 'そちら', 'そっち', 'そで', 'それ', 'それぞれ', 'それなり', 'たくさん', 'たち', 'たび', 'ため', 'だめ', 'ちゃ', 'ちゃん', 'てん', 'とおり', 'とき', 'どこ', 'どこか', 'ところ', 'どちら', 'どっか', 'どっち', 'どれ', 'なか', 'なかば', 'なに', 'など', 'なん', 'はじめ', 'はず', 'はるか', 'ひと', 'ひとつ', 'ふく', 'ぶり', 'べつ', 'へん', 'ぺん', 'ほう', 'ほか', 'まさ', 'まし', 'まとも', 'まま', 'みたい', 'みつ', 'みなさん', 'みんな', 'もと', 'もの', 'もん', 'やつ', 'よう', 'よそ', 'わけ', 'わたし', 'ハイ', '上', '中', '下', '字', '年', '月', '日', '時', '分', '秒', '週', '火', '水', '木', '金', '土', '国', '都', '道', '府', '県', '市', '区', '町', '村', '各', '第', '方', '何', '的', '度', '文', '者', '性', '体', '人', '他', '今', '部', '課', '係', '外', '類', '達', '気', '室', '口', '誰', '用', '界', '会', '首', '男', '女', '別', '話', '私', '屋', '店', '家', '場', '等', '見', '際', '観', '段', '略', '例', '系', '論', '形', '間', '地', '員', '線', '点', '書', '品', '力', '法', '感', '作', '元', '手', '数', '彼', '彼女', '子', '内', '楽', '喜', '怒', '哀', '輪', '頃', '化', '境', '俺', '奴', '高', '校', '婦', '伸', '紀', '誌', 'レ', '行', '列', '事', '士', '台', '集', '様', '所', '歴', '器', '名', '情', '連', '毎', '式', '簿', '回', '匹', '個', '席', '束', '歳', '目', '通', '面', '円', '玉', '枚', '前', '後', '左', '右', '次', '先', '春', '夏', '秋', '冬', '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '百', '千', '万', '億', '兆', '下記', '上記', '時間', '今回', '前回', '場合', '一つ', '年生', '自分', 'ヶ所', 'ヵ所', 'カ所', '箇所', 'ヶ月', 'ヵ月', 'カ月', '箇月', '名前', '本当', '確か', '時点', '全部', '関係', '近く', '方法', '我々', '違い', '多く', '扱い', '新た', 'その後', '半ば', '結局', '様々', '以前', '以後', '以降', '未満', '以上', '以下', '幾つ', '毎日', '自体', '向こう', '何人', '手段', '同じ', '感じ']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(slothlib_stopwords)\n",
    "print('こんにちは' not in slothlib_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金曜日の朝、ケンタッキー州の工業団地で巨大な火事が発生し、当局が被害を封じ込めようと働いたため、その地域に濃い煙が噴出した。市の緊急管理機関のマイク・ワイマー氏によると、ルイビルのゼネラル・エレクトリック・アプライアンス・パークで午前7時少し前に火災が始まりました。 彼は、負傷者や閉じ込められた者の報告はないと言った。 ビデオは煙と明るいオレンジ色の炎の両方を示しました。 消防士は影響を受けた建物の周りの位置を取り、周辺から水を噴霧しました。 ワイマーは、当局が火災の原因を知らなかったとCNNに語り、少なくとも4つの警報が鳴った。 GEのWebサイトによると、ルイビルアプライアンスパークの施設は、米国の製造業を活性化しています。 公園は大きく、34のサッカー場が施設内の倉庫の1つに収まるようになっています。\n",
      "358\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "from parse_5w1h import parse_5w1h\n",
    "#テストファイル読み込み&下処理\n",
    "path = \"/Users/shota/Documents/itac/test_summary1.txt\"\n",
    "with open(path) as f:\n",
    "    text = f.read()\n",
    "text = '金曜日の朝、ケンタッキー州の工業団地で巨大な火事が発生し、当局が被害を封じ込めようと働いたため、その地域に濃い煙が噴出した。市の緊急管理機関のマイク・ワイマー氏によると、ルイビルのゼネラル・エレクトリック・アプライアンス・パークで午前7時少し前に火災が始まりました。 彼は、負傷者や閉じ込められた者の報告はないと言った。 ビデオは煙と明るいオレンジ色の炎の両方を示しました。 消防士は影響を受けた建物の周りの位置を取り、周辺から水を噴霧しました。 ワイマーは、当局が火災の原因を知らなかったとCNNに語り、少なくとも4つの警報が鳴った。 GEのWebサイトによると、ルイビルアプライアンスパークの施設は、米国の製造業を活性化しています。 公園は大きく、34のサッカー場が施設内の倉庫の1つに収まるようになっています。'\n",
    "text = text.replace('\\n','')\n",
    "\n",
    "print(text)\n",
    "doc = nlp(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def increment_edge (graph, node0, node1):\n",
    "    print(\"link {} {}\".format(node0, node1))\n",
    "    \n",
    "    if graph.has_edge(node0, node1):\n",
    "        graph[node0][node1][\"weight\"] += 1.0\n",
    "    else:\n",
    "        graph.add_edge(node0, node1, weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
    "\n",
    "def link_sentence (doc, sent, lemma_graph, seen_lemma):\n",
    "    visited_tokens = []\n",
    "    visited_nodes = []\n",
    "\n",
    "    for i in range(sent.start, sent.end):\n",
    "        token = doc[i]\n",
    "\n",
    "        if token.pos_ in POS_KEPT and token.lemma_ not in slothlib_stopwords:\n",
    "            #token.lemma_は原型. token.pos_は品詞\n",
    "            key = (token.lemma_, token.pos_)\n",
    "\n",
    "            if key not in seen_lemma:\n",
    "                seen_lemma[key] = set([token.i])\n",
    "            else:\n",
    "                seen_lemma[key].add(token.i)\n",
    "\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "\n",
    "            if not node_id in lemma_graph:\n",
    "                lemma_graph.add_node(node_id)\n",
    "\n",
    "            print(\"visit {} {}\".format(visited_tokens, visited_nodes))\n",
    "            print(\"range {}\".format(list(range(len(visited_tokens) - 1, -1, -1))))\n",
    "            \n",
    "            for prev_token in range(len(visited_tokens) - 1, -1, -1):\n",
    "                print(\"prev_tok {} {}\".format(prev_token, (token.i - visited_tokens[prev_token])))\n",
    "                \n",
    "                if (token.i - visited_tokens[prev_token]) <= 3:\n",
    "                    increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            print(\" -- {} {} {} {} {} {}\".format(token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes))\n",
    "\n",
    "            visited_tokens.append(token.i)\n",
    "            visited_nodes.append(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visit [] []\n",
      "range []\n",
      " -- 0 金曜日 金曜日 NOUN [] []\n",
      "visit [0] [0]\n",
      "range [0]\n",
      "prev_tok 0 2\n",
      "link 1 0\n",
      " -- 2 朝 朝 NOUN [0] [0]\n",
      "visit [0, 2] [0, 1]\n",
      "range [1, 0]\n",
      "prev_tok 1 2\n",
      "link 2 1\n",
      "prev_tok 0 4\n",
      " -- 4 ケンタッキー州 ケンタッキー州 PROPN [0, 2] [0, 1]\n",
      "visit [0, 2, 4] [0, 1, 2]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 2\n",
      "link 3 2\n",
      "prev_tok 1 4\n",
      " -- 6 工業 工業 NOUN [0, 2, 4] [0, 1, 2]\n",
      "visit [0, 2, 4, 6] [0, 1, 2, 3]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 1\n",
      "link 4 3\n",
      "prev_tok 2 3\n",
      "link 4 2\n",
      "prev_tok 1 5\n",
      " -- 7 団地 団地 NOUN [0, 2, 4, 6] [0, 1, 2, 3]\n",
      "visit [0, 2, 4, 6, 7] [0, 1, 2, 3, 4]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 2\n",
      "link 5 4\n",
      "prev_tok 3 3\n",
      "link 5 3\n",
      "prev_tok 2 5\n",
      " -- 9 巨大 巨大 ADJ [0, 2, 4, 6, 7] [0, 1, 2, 3, 4]\n",
      "visit [0, 2, 4, 6, 7, 9] [0, 1, 2, 3, 4, 5]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 6 5\n",
      "prev_tok 4 4\n",
      " -- 11 火事 火事 NOUN [0, 2, 4, 6, 7, 9] [0, 1, 2, 3, 4, 5]\n",
      "visit [0, 2, 4, 6, 7, 9, 11] [0, 1, 2, 3, 4, 5, 6]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 2\n",
      "link 7 6\n",
      "prev_tok 5 4\n",
      " -- 13 発生 発生 VERB [0, 2, 4, 6, 7, 9, 11] [0, 1, 2, 3, 4, 5, 6]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13] [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "range [7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 7 3\n",
      "link 8 7\n",
      "prev_tok 6 5\n",
      " -- 16 当局 当局 NOUN [0, 2, 4, 6, 7, 9, 11, 13] [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16] [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "range [8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 8 2\n",
      "link 9 8\n",
      "prev_tok 7 5\n",
      " -- 18 被害 被害 NOUN [0, 2, 4, 6, 7, 9, 11, 13, 16] [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "range [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 9 2\n",
      "link 10 9\n",
      "prev_tok 8 4\n",
      " -- 20 封じ込めよう 封じ込める VERB [0, 2, 4, 6, 7, 9, 11, 13, 16, 18] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "range [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 10 2\n",
      "link 11 10\n",
      "prev_tok 9 4\n",
      " -- 22 働い 働く VERB [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "range [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 11 2\n",
      "link 12 11\n",
      "prev_tok 10 4\n",
      " -- 24 ため 為 NOUN [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "range [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 12 3\n",
      "link 13 12\n",
      "prev_tok 11 5\n",
      " -- 27 地域 地域 NOUN [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24, 27] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "range [13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 13 2\n",
      "link 14 13\n",
      "prev_tok 12 5\n",
      " -- 29 濃い 濃い ADJ [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24, 27] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24, 27, 29] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "range [14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 14 1\n",
      "link 15 14\n",
      "prev_tok 13 3\n",
      "link 15 13\n",
      "prev_tok 12 6\n",
      " -- 30 煙 煙 NOUN [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24, 27, 29] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "visit [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24, 27, 29, 30] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "range [15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 15 2\n",
      "link 16 15\n",
      "prev_tok 14 3\n",
      "link 16 14\n",
      "prev_tok 13 5\n",
      " -- 32 噴出 噴出 VERB [0, 2, 4, 6, 7, 9, 11, 13, 16, 18, 20, 22, 24, 27, 29, 30] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "visit [] []\n",
      "range []\n",
      " -- 38 緊急 緊急 NOUN [] []\n",
      "visit [38] [17]\n",
      "range [0]\n",
      "prev_tok 0 1\n",
      "link 18 17\n",
      " -- 39 管理 管理 NOUN [38] [17]\n",
      "visit [38, 39] [17, 18]\n",
      "range [1, 0]\n",
      "prev_tok 1 1\n",
      "link 19 18\n",
      "prev_tok 0 2\n",
      "link 19 17\n",
      " -- 40 機関 機関 NOUN [38, 39] [17, 18]\n",
      "visit [38, 39, 40] [17, 18, 19]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 2\n",
      "link 20 19\n",
      "prev_tok 1 3\n",
      "link 20 18\n",
      "prev_tok 0 4\n",
      " -- 42 マイク マイク PROPN [38, 39, 40] [17, 18, 19]\n",
      "visit [38, 39, 40, 42] [17, 18, 19, 20]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 2\n",
      "link 21 20\n",
      "prev_tok 2 4\n",
      " -- 44 ワイマー ワイマー PROPN [38, 39, 40, 42] [17, 18, 19, 20]\n",
      "visit [38, 39, 40, 42, 44] [17, 18, 19, 20, 21]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 1\n",
      "link 22 21\n",
      "prev_tok 3 3\n",
      "link 22 20\n",
      "prev_tok 2 5\n",
      " -- 45 氏 氏 NOUN [38, 39, 40, 42, 44] [17, 18, 19, 20, 21]\n",
      "visit [38, 39, 40, 42, 44, 45] [17, 18, 19, 20, 21, 22]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 23 22\n",
      "prev_tok 4 3\n",
      "link 23 21\n",
      "prev_tok 3 5\n",
      " -- 47 よる よる VERB [38, 39, 40, 42, 44, 45] [17, 18, 19, 20, 21, 22]\n",
      "visit [38, 39, 40, 42, 44, 45, 47] [17, 18, 19, 20, 21, 22, 23]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 3\n",
      "link 24 23\n",
      "prev_tok 5 5\n",
      " -- 50 ルイビル ルイビル PROPN [38, 39, 40, 42, 44, 45, 47] [17, 18, 19, 20, 21, 22, 23]\n",
      "visit [38, 39, 40, 42, 44, 45, 47, 50] [17, 18, 19, 20, 21, 22, 23, 24]\n",
      "range [7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 7 2\n",
      "link 25 24\n",
      "prev_tok 6 5\n",
      " -- 52 ゼネラル・エレクトリック ゼネラル・エレクトリック PROPN [38, 39, 40, 42, 44, 45, 47, 50] [17, 18, 19, 20, 21, 22, 23, 24]\n",
      "visit [38, 39, 40, 42, 44, 45, 47, 50, 52] [17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "range [8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 8 2\n",
      "link 26 25\n",
      "prev_tok 7 4\n",
      " -- 54 アプライアンス アプライアンス NOUN [38, 39, 40, 42, 44, 45, 47, 50, 52] [17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "visit [38, 39, 40, 42, 44, 45, 47, 50, 52, 54] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "range [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 9 2\n",
      "link 27 26\n",
      "prev_tok 8 4\n",
      " -- 56 パーク パーク NOUN [38, 39, 40, 42, 44, 45, 47, 50, 52, 54] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "visit [38, 39, 40, 42, 44, 45, 47, 50, 52, 54, 56] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "range [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 10 2\n",
      "link 28 27\n",
      "prev_tok 9 4\n",
      " -- 58 午前 午前 NOUN [38, 39, 40, 42, 44, 45, 47, 50, 52, 54, 56] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "visit [38, 39, 40, 42, 44, 45, 47, 50, 52, 54, 56, 58] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "range [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 11 6\n",
      " -- 64 火災 火災 NOUN [38, 39, 40, 42, 44, 45, 47, 50, 52, 54, 56, 58] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "visit [38, 39, 40, 42, 44, 45, 47, 50, 52, 54, 56, 58, 64] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "range [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 12 2\n",
      "link 30 29\n",
      "prev_tok 11 8\n",
      " -- 66 始まり 始まる VERB [38, 39, 40, 42, 44, 45, 47, 50, 52, 54, 56, 58, 64] [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "visit [] []\n",
      "range []\n",
      " -- 73 負傷者 負傷者 NOUN [] []\n",
      "visit [73] [31]\n",
      "range [0]\n",
      "prev_tok 0 2\n",
      "link 32 31\n",
      " -- 75 閉じ込め 閉じ込める VERB [73] [31]\n",
      "visit [73, 75] [31, 32]\n",
      "range [1, 0]\n",
      "prev_tok 1 5\n",
      " -- 80 報告 報告 NOUN [73, 75] [31, 32]\n",
      "visit [73, 75, 80] [31, 32, 33]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 2\n",
      "link 34 33\n",
      "prev_tok 1 7\n",
      " -- 82 ない 無い ADJ [73, 75, 80] [31, 32, 33]\n",
      "visit [73, 75, 80, 82] [31, 32, 33, 34]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 2\n",
      "link 35 34\n",
      "prev_tok 2 4\n",
      " -- 84 言っ 言う VERB [73, 75, 80, 82] [31, 32, 33, 34]\n",
      "visit [] []\n",
      "range []\n",
      " -- 87 ビデオ ビデオ NOUN [] []\n",
      "visit [87] [36]\n",
      "range [0]\n",
      "prev_tok 0 2\n",
      "link 15 36\n",
      " -- 89 煙 煙 NOUN [87] [36]\n",
      "visit [87, 89] [36, 15]\n",
      "range [1, 0]\n",
      "prev_tok 1 2\n",
      "link 37 15\n",
      "prev_tok 0 4\n",
      " -- 91 明るい 明るい ADJ [87, 89] [36, 15]\n",
      "visit [87, 89, 91] [36, 15, 37]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 1\n",
      "link 38 37\n",
      "prev_tok 1 3\n",
      "link 38 15\n",
      "prev_tok 0 5\n",
      " -- 92 オレンジ オレンジ NOUN [87, 89, 91] [36, 15, 37]\n",
      "visit [87, 89, 91, 92] [36, 15, 37, 38]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 1\n",
      "link 39 38\n",
      "prev_tok 2 2\n",
      "link 39 37\n",
      "prev_tok 1 4\n",
      " -- 93 色 色 NOUN [87, 89, 91, 92] [36, 15, 37, 38]\n",
      "visit [87, 89, 91, 92, 93] [36, 15, 37, 38, 39]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 2\n",
      "link 40 39\n",
      "prev_tok 3 3\n",
      "link 40 38\n",
      "prev_tok 2 4\n",
      " -- 95 炎 炎 NOUN [87, 89, 91, 92, 93] [36, 15, 37, 38, 39]\n",
      "visit [87, 89, 91, 92, 93, 95] [36, 15, 37, 38, 39, 40]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 41 40\n",
      "prev_tok 4 4\n",
      " -- 97 両方 両方 NOUN [87, 89, 91, 92, 93, 95] [36, 15, 37, 38, 39, 40]\n",
      "visit [87, 89, 91, 92, 93, 95, 97] [36, 15, 37, 38, 39, 40, 41]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 2\n",
      "link 42 41\n",
      "prev_tok 5 4\n",
      " -- 99 示し 示す VERB [87, 89, 91, 92, 93, 95, 97] [36, 15, 37, 38, 39, 40, 41]\n",
      "visit [] []\n",
      "range []\n",
      " -- 103 消防士 消防士 NOUN [] []\n",
      "visit [103] [43]\n",
      "range [0]\n",
      "prev_tok 0 2\n",
      "link 44 43\n",
      " -- 105 影響 影響 NOUN [103] [43]\n",
      "visit [103, 105] [43, 44]\n",
      "range [1, 0]\n",
      "prev_tok 1 2\n",
      "link 45 44\n",
      "prev_tok 0 4\n",
      " -- 107 受け 受ける VERB [103, 105] [43, 44]\n",
      "visit [103, 105, 107] [43, 44, 45]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 2\n",
      "link 46 45\n",
      "prev_tok 1 4\n",
      " -- 109 建物 建物 NOUN [103, 105, 107] [43, 44, 45]\n",
      "visit [103, 105, 107, 109] [43, 44, 45, 46]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 2\n",
      "link 47 46\n",
      "prev_tok 2 4\n",
      " -- 111 周り 周り NOUN [103, 105, 107, 109] [43, 44, 45, 46]\n",
      "visit [103, 105, 107, 109, 111] [43, 44, 45, 46, 47]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 2\n",
      "link 48 47\n",
      "prev_tok 3 4\n",
      " -- 113 位置 位置 NOUN [103, 105, 107, 109, 111] [43, 44, 45, 46, 47]\n",
      "visit [103, 105, 107, 109, 111, 113] [43, 44, 45, 46, 47, 48]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 49 48\n",
      "prev_tok 4 4\n",
      " -- 115 取り 取る VERB [103, 105, 107, 109, 111, 113] [43, 44, 45, 46, 47, 48]\n",
      "visit [103, 105, 107, 109, 111, 113, 115] [43, 44, 45, 46, 47, 48, 49]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 2\n",
      "link 50 49\n",
      "prev_tok 5 4\n",
      " -- 117 周辺 周辺 NOUN [103, 105, 107, 109, 111, 113, 115] [43, 44, 45, 46, 47, 48, 49]\n",
      "visit [103, 105, 107, 109, 111, 113, 115, 117] [43, 44, 45, 46, 47, 48, 49, 50]\n",
      "range [7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 7 4\n",
      " -- 121 噴霧 噴霧 VERB [103, 105, 107, 109, 111, 113, 115, 117] [43, 44, 45, 46, 47, 48, 49, 50]\n",
      "visit [] []\n",
      "range []\n",
      " -- 126 ワイマー ワイマー NOUN [] []\n",
      "visit [126] [52]\n",
      "range [0]\n",
      "prev_tok 0 3\n",
      "link 8 52\n",
      " -- 129 当局 当局 NOUN [126] [52]\n",
      "visit [126, 129] [52, 8]\n",
      "range [1, 0]\n",
      "prev_tok 1 2\n",
      "link 29 8\n",
      "prev_tok 0 5\n",
      " -- 131 火災 火災 NOUN [126, 129] [52, 8]\n",
      "visit [126, 129, 131] [52, 8, 29]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 2\n",
      "link 53 29\n",
      "prev_tok 1 4\n",
      " -- 133 原因 原因 NOUN [126, 129, 131] [52, 8, 29]\n",
      "visit [126, 129, 131, 133] [52, 8, 29, 53]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 2\n",
      "link 54 53\n",
      "prev_tok 2 4\n",
      " -- 135 知ら 知る VERB [126, 129, 131, 133] [52, 8, 29, 53]\n",
      "visit [126, 129, 131, 133, 135] [52, 8, 29, 53, 54]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 4\n",
      " -- 139 CNN CNN PROPN [126, 129, 131, 133, 135] [52, 8, 29, 53, 54]\n",
      "visit [126, 129, 131, 133, 135, 139] [52, 8, 29, 53, 54, 55]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 56 55\n",
      "prev_tok 4 6\n",
      " -- 141 語り 語る VERB [126, 129, 131, 133, 135, 139] [52, 8, 29, 53, 54, 55]\n",
      "visit [126, 129, 131, 133, 135, 139, 141] [52, 8, 29, 53, 54, 55, 56]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 2\n",
      "link 57 56\n",
      "prev_tok 5 4\n",
      " -- 143 少なく 少ない ADJ [126, 129, 131, 133, 135, 139, 141] [52, 8, 29, 53, 54, 55, 56]\n",
      "visit [126, 129, 131, 133, 135, 139, 141, 143] [52, 8, 29, 53, 54, 55, 56, 57]\n",
      "range [7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 7 3\n",
      "link 58 57\n",
      "prev_tok 6 5\n",
      " -- 146 つ つ NOUN [126, 129, 131, 133, 135, 139, 141, 143] [52, 8, 29, 53, 54, 55, 56, 57]\n",
      "visit [126, 129, 131, 133, 135, 139, 141, 143, 146] [52, 8, 29, 53, 54, 55, 56, 57, 58]\n",
      "range [8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 8 2\n",
      "link 59 58\n",
      "prev_tok 7 5\n",
      " -- 148 警報 警報 NOUN [126, 129, 131, 133, 135, 139, 141, 143, 146] [52, 8, 29, 53, 54, 55, 56, 57, 58]\n",
      "visit [126, 129, 131, 133, 135, 139, 141, 143, 146, 148] [52, 8, 29, 53, 54, 55, 56, 57, 58, 59]\n",
      "range [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 9 2\n",
      "link 60 59\n",
      "prev_tok 8 4\n",
      " -- 150 鳴っ 鳴る VERB [126, 129, 131, 133, 135, 139, 141, 143, 146, 148] [52, 8, 29, 53, 54, 55, 56, 57, 58, 59]\n",
      "visit [] []\n",
      "range []\n",
      " -- 153 GE GE PROPN [] []\n",
      "visit [153] [61]\n",
      "range [0]\n",
      "prev_tok 0 2\n",
      "link 62 61\n",
      " -- 155 Webサイト webサイト NOUN [153] [61]\n",
      "visit [153, 155] [61, 62]\n",
      "range [1, 0]\n",
      "prev_tok 1 2\n",
      "link 23 62\n",
      "prev_tok 0 4\n",
      " -- 157 よる よる VERB [153, 155] [61, 62]\n",
      "visit [153, 155, 157] [61, 62, 23]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 3\n",
      "link 24 23\n",
      "prev_tok 1 5\n",
      " -- 160 ルイビル ルイビル PROPN [153, 155, 157] [61, 62, 23]\n",
      "visit [153, 155, 157, 160] [61, 62, 23, 24]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 1\n",
      "link 26 24\n",
      "prev_tok 2 4\n",
      " -- 161 アプライアンス アプライアンス NOUN [153, 155, 157, 160] [61, 62, 23, 24]\n",
      "visit [153, 155, 157, 160, 161] [61, 62, 23, 24, 26]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 1\n",
      "link 27 26\n",
      "prev_tok 3 2\n",
      "link 27 24\n",
      "prev_tok 2 5\n",
      " -- 162 パーク パーク NOUN [153, 155, 157, 160, 161] [61, 62, 23, 24, 26]\n",
      "visit [153, 155, 157, 160, 161, 162] [61, 62, 23, 24, 26, 27]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 63 27\n",
      "prev_tok 4 3\n",
      "link 63 26\n",
      "prev_tok 3 4\n",
      " -- 164 施設 施設 NOUN [153, 155, 157, 160, 161, 162] [61, 62, 23, 24, 26, 27]\n",
      "visit [153, 155, 157, 160, 161, 162, 164] [61, 62, 23, 24, 26, 27, 63]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 3\n",
      "link 64 63\n",
      "prev_tok 5 5\n",
      " -- 167 米国 米国 PROPN [153, 155, 157, 160, 161, 162, 164] [61, 62, 23, 24, 26, 27, 63]\n",
      "visit [153, 155, 157, 160, 161, 162, 164, 167] [61, 62, 23, 24, 26, 27, 63, 64]\n",
      "range [7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 7 2\n",
      "link 65 64\n",
      "prev_tok 6 5\n",
      " -- 169 製造業 製造業 NOUN [153, 155, 157, 160, 161, 162, 164, 167] [61, 62, 23, 24, 26, 27, 63, 64]\n",
      "visit [153, 155, 157, 160, 161, 162, 164, 167, 169] [61, 62, 23, 24, 26, 27, 63, 64, 65]\n",
      "range [8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 8 2\n",
      "link 66 65\n",
      "prev_tok 7 4\n",
      " -- 171 活性化 活性化 VERB [153, 155, 157, 160, 161, 162, 164, 167, 169] [61, 62, 23, 24, 26, 27, 63, 64, 65]\n",
      "visit [] []\n",
      "range []\n",
      " -- 177 公園 公園 NOUN [] []\n",
      "visit [177] [67]\n",
      "range [0]\n",
      "prev_tok 0 2\n",
      "link 68 67\n",
      " -- 179 大きく 大きい ADJ [177] [67]\n",
      "visit [177, 179] [67, 68]\n",
      "range [1, 0]\n",
      "prev_tok 1 4\n",
      " -- 183 サッカー サッカー NOUN [177, 179] [67, 68]\n",
      "visit [177, 179, 183] [67, 68, 69]\n",
      "range [2, 1, 0]\n",
      "prev_tok 2 3\n",
      "link 70 69\n",
      "prev_tok 1 7\n",
      " -- 186 施設内 施設内 NOUN [177, 179, 183] [67, 68, 69]\n",
      "visit [177, 179, 183, 186] [67, 68, 69, 70]\n",
      "range [3, 2, 1, 0]\n",
      "prev_tok 3 2\n",
      "link 71 70\n",
      "prev_tok 2 5\n",
      " -- 188 倉庫 倉庫 NOUN [177, 179, 183, 186] [67, 68, 69, 70]\n",
      "visit [177, 179, 183, 186, 188] [67, 68, 69, 70, 71]\n",
      "range [4, 3, 2, 1, 0]\n",
      "prev_tok 4 3\n",
      "link 58 71\n",
      "prev_tok 3 5\n",
      " -- 191 つ つ NOUN [177, 179, 183, 186, 188] [67, 68, 69, 70, 71]\n",
      "visit [177, 179, 183, 186, 188, 191] [67, 68, 69, 70, 71, 58]\n",
      "range [5, 4, 3, 2, 1, 0]\n",
      "prev_tok 5 2\n",
      "link 72 58\n",
      "prev_tok 4 5\n",
      " -- 193 収まる 収まる VERB [177, 179, 183, 186, 188, 191] [67, 68, 69, 70, 71, 58]\n",
      "visit [177, 179, 183, 186, 188, 191, 193] [67, 68, 69, 70, 71, 58, 72]\n",
      "range [6, 5, 4, 3, 2, 1, 0]\n",
      "prev_tok 6 3\n",
      "link 73 72\n",
      "prev_tok 5 5\n",
      " -- 196 なっ 成る VERB [177, 179, 183, 186, 188, 191, 193] [67, 68, 69, 70, 71, 58, 72]\n"
     ]
    }
   ],
   "source": [
    "lemma_graph = nx.Graph()\n",
    "seen_lemma = {}\n",
    "\n",
    "for sent in doc.sents:\n",
    "    link_sentence(doc, sent, lemma_graph, seen_lemma)\n",
    "    #break # only test one sentence\n",
    "labels = {}\n",
    "keys = list(seen_lemma.keys())\n",
    "\n",
    "for i in range(len(seen_lemma)):\n",
    "    labels[i] = keys[i][0].lower()\n",
    "ranks = nx.pagerank(lemma_graph)\n",
    "imp_list = []\n",
    "for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
    "    imp_list.append(labels[node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def collect_5w1hphrases (chunk,chunk_start,chunk_end, phrases, counts):\n",
    "    chunk_len = chunk_end - chunk_start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk_start, chunk_end):\n",
    "        token = doc[i]\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        \n",
    "        if key in seen_lemma:\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "            rank = ranks[node_id]\n",
    "            sq_sum_rank += rank\n",
    "            compound_key.add(key)\n",
    "        \n",
    "            #print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "        else:\n",
    "            non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "    #print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk, chunk_start, chunk_end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_5w1h import parse_5w1h\n",
    "parse = parse_5w1h(0)\n",
    "parse.extract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金曜日の朝、ケンタッキー州の工業団地で巨大な火事が\n",
      "Who\n",
      "\n",
      "\n",
      "発生し、\n",
      "How\n",
      "\n",
      "\n",
      "当局が\n",
      "Who\n",
      "\n",
      "\n",
      "被害を\n",
      "What\n",
      "\n",
      "\n",
      "封じ込めようと働いた\n",
      "How\n",
      "\n",
      "\n",
      "ため、\n",
      "Why\n",
      "\n",
      "\n",
      "その地域に濃い\n",
      "How\n",
      "\n",
      "\n",
      "煙が\n",
      "Who\n",
      "\n",
      "\n",
      "噴出した。\n",
      "How\n",
      "\n",
      "\n",
      "市の緊急管理機関のマイク・\n",
      "None\n",
      "\n",
      "\n",
      "ワイマー氏によると、\n",
      "How\n",
      "\n",
      "\n",
      "ルイビルのゼネラル・エレクトリック・アプライアンス・\n",
      "None\n",
      "\n",
      "\n",
      "パークで午前7時\n",
      "When\n",
      "\n",
      "\n",
      "少し前に火災が\n",
      "Who\n",
      "\n",
      "\n",
      "始まりました。\n",
      "How\n",
      "\n",
      "\n",
      "彼は、負傷者や閉じ込められた者の報告は\n",
      "Who\n",
      "\n",
      "\n",
      "ないと言った。\n",
      "How\n",
      "\n",
      "\n",
      "ビデオは\n",
      "Who\n",
      "\n",
      "\n",
      "煙と明るいオレンジ色の炎の両方を\n",
      "What\n",
      "\n",
      "\n",
      "示しました。\n",
      "How\n",
      "\n",
      "\n",
      "消防士は\n",
      "Who\n",
      "\n",
      "\n",
      "影響を\n",
      "What\n",
      "\n",
      "\n",
      "受けた\n",
      "How\n",
      "\n",
      "\n",
      "建物の周りの位置を\n",
      "What\n",
      "\n",
      "\n",
      "取り、\n",
      "How\n",
      "\n",
      "\n",
      "周辺から水を\n",
      "What\n",
      "\n",
      "\n",
      "噴霧しました。\n",
      "How\n",
      "\n",
      "\n",
      "ワイマーは、\n",
      "None\n",
      "\n",
      "\n",
      "当局が\n",
      "Who\n",
      "\n",
      "\n",
      "火災の原因を\n",
      "What\n",
      "\n",
      "\n",
      "知らなかったとCNNに語り、少なくとも\n",
      "How\n",
      "\n",
      "\n",
      "4つの警報が\n",
      "Who\n",
      "\n",
      "\n",
      "鳴った。\n",
      "How\n",
      "\n",
      "\n",
      "GEのWebサイトによると、\n",
      "How\n",
      "\n",
      "\n",
      "ルイビルアプライアンスパークの施設は、\n",
      "Who\n",
      "\n",
      "\n",
      "米国の\n",
      "Where\n",
      "\n",
      "\n",
      "製造業を\n",
      "What\n",
      "\n",
      "\n",
      "活性化しています。\n",
      "How\n",
      "\n",
      "\n",
      "公園は\n",
      "Who\n",
      "\n",
      "\n",
      "大きく、\n",
      "How\n",
      "\n",
      "\n",
      "34のサッカー場が\n",
      "Who\n",
      "\n",
      "\n",
      "施設内の倉庫の1つに収まるようになっています。\n",
      "How\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parse.display_5w1h()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phrases = {}\n",
    "counts = {}\n",
    "\n",
    "for i,chunk in enumerate(parse._5w1hs):\n",
    "\n",
    "     collect_5w1hphrases(chunk, parse._5w1h_s[i],parse._5w1h_e[i],phrases, counts)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parse._5w1h_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金曜日の朝、ケンタッキー州の工業団地で巨大な火事が\n",
      "発生し、\n",
      "当局が\n",
      "被害を\n",
      "封じ込めようと働いた\n",
      "ため、\n",
      "その地域に濃い\n",
      "煙が\n",
      "噴出した。\n",
      "市の緊急管理機関のマイク・\n",
      "ワイマー氏によると、\n",
      "ルイビルのゼネラル・エレクトリック・アプライアンス・\n",
      "パークで午前7時\n",
      "少し前に火災が\n",
      "始まりました。\n",
      "彼は、負傷者や閉じ込められた者の報告は\n",
      "ないと言った。\n",
      "ビデオは\n",
      "煙と明るいオレンジ色の炎の両方を\n",
      "示しました。\n",
      "消防士は\n",
      "影響を\n",
      "受けた\n",
      "建物の周りの位置を\n",
      "取り、\n",
      "周辺から水を\n",
      "噴霧しました。\n",
      "ワイマーは、\n",
      "火災の原因を\n",
      "知らなかったとCNNに語り、少なくとも\n",
      "4つの警報が\n",
      "鳴った。\n",
      "GEのWebサイトによると、\n",
      "ルイビルアプライアンスパークの施設は、\n",
      "米国の\n",
      "製造業を\n",
      "活性化しています。\n",
      "公園は\n",
      "大きく、\n",
      "34のサッカー場が\n",
      "施設内の倉庫の1つに収まるようになっています。\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "min_ranks = {}\n",
    "\n",
    "for compound_key, rank_tuples in phrases.items():\n",
    "    l = list(rank_tuples)\n",
    "    l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    phrase, rank = l[0]\n",
    "    print(phrase)\n",
    "    #count = counts[compound_key]\n",
    "    \n",
    "    #min_phrases[phrase] = (rank, count)\n",
    "    min_ranks[phrase] = (rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金曜日の朝、ケンタッキー州の工業団地で巨大な火事が\n",
      "Who\n",
      "当局が\n",
      "Who\n",
      "その地域に濃い\n",
      "How\n",
      "煙が\n",
      "Who\n",
      "市の緊急管理機関のマイク・\n",
      "None\n",
      "ワイマー氏によると、\n",
      "How\n",
      "ルイビルのゼネラル・エレクトリック・アプライアンス・\n",
      "None\n",
      "煙と明るいオレンジ色の炎の両方を\n",
      "What\n",
      "影響を\n",
      "What\n",
      "受けた\n",
      "How\n",
      "建物の周りの位置を\n",
      "What\n",
      "取り、\n",
      "How\n",
      "当局が\n",
      "Who\n",
      "火災の原因を\n",
      "What\n",
      "4つの警報が\n",
      "Who\n",
      "ルイビルアプライアンスパークの施設は、\n",
      "Who\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "summary = []\n",
    "while i < len(parse._5w1hs):\n",
    "    if parse._5w1hs[i] in min_ranks:\n",
    "        if min_ranks[parse._5w1hs[i]] > 0.03:\n",
    "            print(parse._5w1hs[i])\n",
    "            print(parse._5w1h_type[i])\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "煙が噴出した。\n",
      "煙と明るいオレンジ色の炎の両方を示しました。\n",
      "ルイビルアプライアンスパークの施設は、米国の製造業を活性化しています。\n"
     ]
    }
   ],
   "source": [
    "#####要約最新#####\n",
    "i = 0 \n",
    "summary = []\n",
    "count = []\n",
    "while i < len(parse._5w1hs):\n",
    "    if parse._5w1hs[i] in min_ranks:\n",
    "        if min_ranks[parse._5w1hs[i]] > 0.04:\n",
    "            if parse._5w1hs[i] not in count:\n",
    "                summary.append(parse._5w1hs[i])\n",
    "                count.append(parse._5w1hs[i])\n",
    "                \n",
    "            if parse._5w1h_type[i] ==\"How\":\n",
    "                l = i-1\n",
    "                while l >= 0:\n",
    "                    if parse._5w1hs[l] not in count:\n",
    "                        summary.insert(0,parse._5w1hs[l])\n",
    "                        count.append(parse._5w1hs[l])\n",
    "                    if parse._5w1h_type[l] == 'Who'or parse._5w1h_type[l] == 'What':\n",
    "                        if summary != []:\n",
    "                            print(''.join(summary))\n",
    "                        summary = []\n",
    "                        break\n",
    "                    l -= 1\n",
    "            else:\n",
    "                l = i+1\n",
    "                while l < len(parse._5w1hs):\n",
    "                    if parse._5w1hs[l] not in count:\n",
    "                        summary.append(parse._5w1hs[l])\n",
    "                        count.append(parse._5w1hs[l])\n",
    "                    if parse._5w1h_type[l] == 'How':\n",
    "                        if summary != []:\n",
    "                            print(''.join(summary))\n",
    "                        summary = []\n",
    "                        break\n",
    "                    l += 1\n",
    "                \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-10762eeb23da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmin_phrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'min_phrases' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "summary = []\n",
    "for phrase, (rank, start,end,_type) in min_phrases.items():\n",
    "\n",
    "    if rank > 0.04:\n",
    "        summary.append(phrase)\n",
    "        if parse.doc[end]._._5w1h == 'How':\n",
    "            l = start-1\n",
    "            while l > 0:\n",
    "                if parse.doc[l]._._5w1h == 'Who'or parse.doc[l]._._5w1h == 'What':\n",
    "                    A = parse.doc[l]._._5w1h\n",
    "                    while l > 0:\n",
    "                        summary.insert(0,parse.doc[l].text)\n",
    "                   \n",
    "                        l -=1\n",
    "                        if parse.doc[l]._._5w1h != A:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "                else:\n",
    "                    summary.insert(0,parse.doc[l].text)\n",
    "                    l -= 1\n",
    "        else:\n",
    "            l = end-1\n",
    "            while l < len(parse.doc)-1:\n",
    "                if parse.doc[l]._._5w1h == 'How':\n",
    "                    while l< len(parse.doc)-1:\n",
    "                        summary.append(parse.doc[l].text)\n",
    "                        summary.append(' ')\n",
    "                        l += 1\n",
    "                        if parse.doc[l]._._5w1h != 'How':\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    summary.append(parse.doc[l].text)\n",
    "                    l += 1\n",
    "\n",
    "        print(''.join(summary))\n",
    "        summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-71e547128129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_5w1hphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_5w1h_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_5w1h_e\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(parse.doc[parse._5w1h_e[i]-1]._._5w1h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "for i,chunk in enumerate(parse._5w1hs):\n",
    "    summary = []\n",
    "    rank = collect_5w1hphrases(chunk, parse._5w1h_s[i],parse._5w1h_e[i],phrases, counts)\n",
    "    #print(parse.doc[parse._5w1h_e[i]-1]._._5w1h)\n",
    "    if rank > 0.05:\n",
    "        summary.append(chunk)\n",
    "        print(chunk)\n",
    "        print(parse.doc[parse._5w1h_e[i]]._._5w1h)\n",
    "        if parse.doc[parse._5w1h_e[i]]._._5w1h == 'How':\n",
    "            l = i\n",
    "            while l > 0:\n",
    "                \n",
    "                summary.append(parse._5w1hs[l-1])\n",
    "                if parse.doc[parse._5w1h_e[l-1]]._._5w1h == 'Who'or parse.doc[parse._5w1h_e[l-1]]._._5w1h == 'What':\n",
    "                    break\n",
    "                l=l-1\n",
    "        else:\n",
    "            l = i\n",
    "            while l < len(parse._5w1hs)-1:\n",
    "                summary.append(parse._5w1hs[l+1])\n",
    "                if parse.doc[parse._5w1h_e[l+1]]._._5w1h == 'How':\n",
    "                    break\n",
    "                l=l+1\n",
    "                \n",
    "        #print(''.join(summary))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "厳しいという\n",
      "日本\n",
      "swiftで万歩計\n",
      "テクト条件で作業が\n",
      "ちょっとあるお酒持ってもらっていい。\n",
      "テクトって\n",
      "ITDは\n",
      "AI専用のホームページの話が\n",
      "OKえっとー聞こえる。\n",
      "尾崎は2番目がアイタイムズで、ITDの件は\n"
     ]
    }
   ],
   "source": [
    "for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)[0:10]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKえっとー聞こえる。\n",
      "0.030184080526018446\n",
      "はい聞こえます。\n",
      "0.016637410546873434\n",
      "AI専用のホームページの話が\n",
      "0.03384426230097155\n",
      "あったじゃん。\n",
      "0.02671978258178838\n",
      "なるほど。\n",
      "0.0\n",
      "あれさあ尾崎の方でもってもらいたいの。\n",
      "0.01593702872572663\n",
      "承知しました。\n",
      "0.005532833351724883\n",
      "多分出来上がって石井さんから入れてもらってるんだけどあれおすすめらんないかな。\n",
      "0.017911228923577684\n",
      "請求\n",
      "0.020619652471058063\n",
      "さっさとホームページ出してほしいんじゃないのページを公開を\n",
      "0.01998657693433237\n",
      "ちょっとあるお酒持ってもらっていい。\n",
      "0.04186219171168881\n",
      "色々あるから大変だったらちょっと俺に言って。\n",
      "0.023812464272063814\n",
      "塩釜も\n",
      "0.019216051434616046\n",
      "あるしアイタムズのさっき言ったようにちょっと持ってるじゃん。\n",
      "0.025448645986879587\n",
      "ITDは\n",
      "0.035118604553101104\n",
      "伊藤にさあ引き継いでいいから。\n",
      "0.026993490574584528\n",
      "ITD伊藤に引き継いだ後でもいいんだけどでいいから\n",
      "0.025336208858432047\n",
      "AIのホームページのやつ入ってほしいからだから\n",
      "0.02235585375423188\n",
      "アイタムズ今月、5月も\n",
      "0.011122471777220536\n",
      "少しあるのかな、塩釜メインでそういう感じでいい、ちょっとお願いして。\n",
      "0.029065711104206946\n",
      "一点だけですね、ご報告したいことがありまして。\n",
      "0.013016158976702618\n",
      "どこか明日の10時まで\n",
      "0.004453426331346783\n",
      "日本\n",
      "0.05350405160091995\n",
      "テクト条件で作業が\n",
      "0.0421354959441942\n",
      "厳しいという\n",
      "0.05503136173256686\n",
      "伺ったんですね。\n",
      "0.008565213153235888\n",
      "入れるしかないと考えております。\n",
      "0.010719390825437578\n",
      "テクトって\n",
      "0.03660930421641984\n",
      "そんな忙しいんだ。\n",
      "0.013415514917528355\n",
      "IBMのもやってるからね。\n",
      "0.016870624749047505\n",
      "中島さんも\n",
      "0.007936507936507938\n",
      "ほぼ同じ状況のようです。\n",
      "0.004453426331346783\n",
      "かといって誰入れる。\n",
      "0.02533733221951164\n",
      "JAVAで岡島さん作ってんでしょ。\n",
      "0.021262521642281294\n",
      "言語ちょっと確認して。\n",
      "0.021068745701976314\n",
      "PHPです。\n",
      "0.014477196828075469\n",
      "で作ってるって。\n",
      "0.012539902061529017\n",
      "できるけど忙しいから。\n",
      "0.020607562828866575\n",
      "PHPだったらでもまだ見つかるね。\n",
      "0.00882751746147155\n",
      "ちょっとら当たる。\n",
      "0.006443641397205646\n",
      "もう一つがですね\n",
      "0.004910463758239914\n",
      "swiftで万歩計\n",
      "0.051489995476317545\n",
      "アプリって\n",
      "0.029097333409775893\n",
      "かかるれている。\n",
      "0.012332915073778533\n",
      "とさんできる。\n",
      "0.012052650070722606\n",
      "さん監修の元とさんって\n",
      "0.004453426331346783\n",
      "PHPが尾崎の動きの方が\n",
      "0.02372859251266789\n",
      "好きでいいと思うんだよ。\n",
      "0.01917063040066117\n",
      "最もマルチプラットフォームだと望ましいんですよ。\n",
      "0.019506784252876185\n",
      "一旦もういいよ。\n",
      "0.01750139446532479\n",
      "りょうさん空くまでIOSでとさんやってもらって、Androidの対応も\n",
      "0.023213792076229495\n",
      "最悪Androidで出てくるけど後でフラットにする。\n",
      "0.022111018807502096\n",
      "あのグループで今から\n",
      "0.005532833351724883\n",
      "投げるからちょっと待ってここ\n",
      "0.021068745701976314\n",
      "投げた。\n",
      "0.020491975376758818\n",
      "優先順位としては塩釜一番なのは変わりないから\n",
      "0.015556334590441345\n",
      "尾崎は2番目がアイタイムズで、ITDの件は\n",
      "0.029150574865790403\n",
      "3番目。\n",
      "0.021685411085986036\n",
      "終わってからでいい、AIのホームページは。\n",
      "0.025018799553668918\n",
      "忘れないでね。\n",
      "0.005532833351724883\n",
      "早く会いたいですね先週だっけ石井さんから言われてるやつごめんちょっとそれは\n",
      "0.022425020218192333\n"
     ]
    }
   ],
   "source": [
    "for phrase, (rank, count) in min_phrases.items():\n",
    "    print(phrase)\n",
    "    print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(min_phrases.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_phrases.items()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def collect_phrases (chunk, phrases, counts):\n",
    "    chunk_len = chunk.end - chunk.start + 1\n",
    "    sq_sum_rank = 0.0\n",
    "    non_lemma = 0\n",
    "    compound_key = set([])\n",
    "\n",
    "    for i in range(chunk.start, chunk.end):\n",
    "        token = doc[i]\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        \n",
    "        if key in seen_lemma:\n",
    "            node_id = list(seen_lemma.keys()).index(key)\n",
    "            rank = ranks[node_id]\n",
    "            sq_sum_rank += rank\n",
    "            compound_key.add(key)\n",
    "        \n",
    "            print(\" {} {} {} {}\".format(token.lemma_, token.pos_, node_id, rank))\n",
    "        else:\n",
    "            non_lemma += 1\n",
    "    \n",
    "    # although the noun chunking is greedy, we discount the ranks using a\n",
    "    # point estimate based on the number of non-lemma tokens within a phrase\n",
    "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
    "\n",
    "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
    "    phrase_rank *= non_lemma_discount\n",
    "\n",
    "    # remove spurious punctuation\n",
    "    phrase = chunk.text.lower().replace(\"'\", \"\")\n",
    "\n",
    "    # create a unique key for the the phrase based on its lemma components\n",
    "    compound_key = tuple(sorted(list(compound_key)))\n",
    "    \n",
    "    if not compound_key in phrases:\n",
    "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
    "        counts[compound_key] = 1\n",
    "    else:\n",
    "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
    "        counts[compound_key] += 1\n",
    "\n",
    "    print(\"{} {} {} {} {} {}\".format(phrase_rank, chunk.text, chunk.start, chunk.end, chunk_len, counts[compound_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = {}\n",
    "counts = {}\n",
    "for sent in doc.sents:\n",
    "    collect_phrases(sent, phrases, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "min_phrases = {}\n",
    "\n",
    "for compound_key, rank_tuples in phrases.items():\n",
    "    l = list(rank_tuples)\n",
    "    l.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    phrase, rank = l[0]\n",
    "    count = counts[compound_key]\n",
    "    \n",
    "    min_phrases[phrase] = (rank, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True)[0:3]:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
