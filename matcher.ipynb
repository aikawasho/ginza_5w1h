{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-6-62b31d8871fb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-62b31d8871fb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    あらすじとレビューを見ていると、殺されると同じ日を何度も繰り返す主人公が犯人を突き止めるホラーサスペンス映画だという印象を受けますが、実際にはホラー要素もサスペンス要素も中途半端な映画です。\u001b[0m\n\u001b[0m                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "あらすじとレビューを見ていると、殺されると同じ日を何度も繰り返す主人公が犯人を突き止めるホラーサスペンス映画だという印象を受けますが、実際にはホラー要素もサスペンス要素も中途半端な映画です。\n",
    "犯人を見つけられず殺されるとゲームオーバー。またその日の朝からやり直し。\n",
    "あらすじにもある通り、ヒロインが自分が殺害される日を延々と繰り返すお話。\n",
    "コースによっては72分とかあるので、炊飯の時間はかかるかな。\n",
    "確かに横長なので、地域によっては不燃ゴミで出せないかも。\n",
    "釜に米がひっつく。とありましたが、確かに米はつきやすいかな。するっとは取れないですね。ちょっとひっかかる。\n",
    "欲を言えば、釜が軽いかな。だからあまり美味しくできなかったのかな。\n",
    "評判を読むと、言うほど美味しくない。というコメントがありましたが、私はおいしいと思いました。\n",
    "値段の割に良い商品だと思います。\n",
    "普通に食べられればよいと思っている人には、この製品で十分だと思います。\n",
    "中蓋の形がシンプルで、なべも中蓋も洗いやすく、また、メーカーの製品仕様等を調べたところ、この製品のなべの中で直接洗米してもよいと書いてあり、洗米も楽です。\n",
    "毎日の炊飯と、炊飯器のお手入れがとにかく楽なものを探していて、この製品に決めました。\n",
    "釜だけ中古品のような感じでしたので　星1つです\n",
    "釜だけすり替えて返品したのでは無いかと思うような商品でした\n",
    "日常生活の都合上、いつ車で出かけなければならないことがあるか判らない。\n",
    "綺麗な本体の中にキズがある釜が入っていた\n",
    "長らくノイズキャンセリングヘッドホンイヤホンを使ってきましたが、新参者のAppleが古参のBOSEやSONYに匹敵する商品を出したことは非常に驚きです。\n",
    "ノイズキャンセリング等は他のユーザーさんが評価されているので割愛します。\n",
    "前にAmazonで買ってレビューを書いたのに消されてる\n",
    "その分大きさも増したが基本的に持ち運ぶものではないので影響は少ない。\n",
    "手入れも洗うところ少ないし、なくなりそうな小さい部品もないし、簡単です。\n",
    "良くなった点は音質がよくなったところ。\n",
    "ので、主に音質についてのレビューです。\n",
    "急ぎで買った物なので返品する余裕も無く、使ってみたら釜にご飯が結構こびりつきました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What あらすじ\n",
      "What と\n",
      "What レビュー\n",
      "What を\n",
      "How 見\n",
      "How て\n",
      "How いる\n",
      "How と\n",
      "How 、\n",
      "Who 殺さ\n",
      "Who れる\n",
      "Who と\n",
      "Who 同じ\n",
      "Who 日\n",
      "Who を\n",
      "Who 何\n",
      "Who 度\n",
      "Who も\n",
      "Who 繰り返す\n",
      "Who 主人公\n",
      "Who が\n",
      "What 犯人\n",
      "What を\n",
      "How 突き止める\n",
      "What ホラー\n",
      "What サスペンス\n",
      "What 映画\n",
      "What だ\n",
      "What と\n",
      "What いう\n",
      "What 印象\n",
      "What を\n",
      "How 受け\n",
      "How ます\n",
      "How が\n",
      "How 、\n",
      "Who 実際\n",
      "Who に\n",
      "Who は\n",
      "Who ホラー\n",
      "Who 要素\n",
      "Who も\n",
      "Who サスペンス\n",
      "Who 要素\n",
      "Who も\n",
      "How 中途半端\n",
      "How な\n",
      "How 映画\n",
      "How です\n",
      "How 。\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "nlp = spacy.load('ja_ginza')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "if not Token.has_extension(\"_5w1h\"):\n",
    "    Token.set_extension(\"_5w1h\", default=None)\n",
    "    \n",
    "\n",
    "Where_pattern1 = [{\"POS\":{\"REGEX\":\"NOUN|PROPN|PRON\"},\"ENT_TYPE\":{\"REGEX\":\"Country|Province\"}}]\n",
    "Where_pattern2 = [{\"TEXT\":{\"REGEX\":\"Amazon\"}}]\n",
    "When_pattern = [{\"TEXT\":\"前\"}]\n",
    "Why_pattern1 = [{\"TEXT\":\"で\"},{\"TEXT\":\"は\"},{\"TEXT\":\"ない\"},{\"TEXT\":\"の\"},{\"TEXT\":\"で\"}]\n",
    "Why_pattern2 = [{\"POS\":{\"REGEX\":\"VERB|AUX|SCONJ\"},\"OP\":\"*\"},{\"TEXT\":\"の\"},{\"TEXT\":\"で\"}]\n",
    "\n",
    "How_pattern1 = [{\"POS\":{\"REGEX\":\"VERB|ADJ|NOUN\"},\"DEP\":{\"REGEX\":\"ROOT\"}}]\n",
    "How_pattern2 = [{\"POS\":{\"REGEX\":\"VERB|ADJ\"},\"OP\":\"+\"},{\"DEP\":{\"REGEX\":\"ROOT|cc\"}}]\n",
    "How_pattern3 = [{\"POS\":{\"REGEX\":\"VERB|ADJ\"},\"OP\":\"+\"},{\"POS\":{\"REGEX\":\"AUX\"},\"OP\":\"?\"},{\"DEP\":{\"REGEX\":\"ROOT\"}}]\n",
    "How_pattern4 = [{\"POS\":{\"REGEX\":\"VERB|ADJ\"},\"OP\":\"+\"},{\"OP\":\"?\"},{\"POS\":\"PUNCT\"}]\n",
    "How_pattern5 = [{\"POS\":{\"REGEX\":\"VERB|ADJ|AUX\"},\"OP\":\"*\"},{\"TEXT\":\"の\"},{\"TEXT\":\"に\"}]\n",
    "How_pattern6 = [{\"DEP\":\"advcl\"},{\"DEP\":\"mark\"}]\n",
    "How_pattern7 = [{\"POS\":\"VERB\"},{\"POS\":\"AUX\",\"OP\":\"+\"},{\"TEXT\":\"が\",\"POS\":\"CCONJ\"}]\n",
    "How_pattern8 = [{\"POS\":\"VERB\"},{\"POS\":\"AUX\"},{\"TEXT\":\"ところ\",\"POS\":\"NOUN\"}]\n",
    "How_pattern9 = [{\"POS\":\"VERB\",\"DEP\":\"advcl\"},{\"TEXT\":\"し\",\"POS\":\"CCONJ\"}]\n",
    "\n",
    "\n",
    "Who_pattern1 = [{\"TEXT\":\"の\",\"OP\":\"?\"},{\"POS\":\"NOUN\",\"DEP\":\"compound\",\"OP\":\"*\"},{\"POS\":{\"REGEX\":\"NOUN|PRON|PROPN\"},\"DEP\":{\"REGEX\":\"iobj|obl|nsubj\"},\"TEXT\":{\"NOT_IN\":[\"度\",\"幾つ\"]}},{\"TEXT\":{\"REGEX\":\"が|は|も\"}}]\n",
    "Who_pattern2 = [{\"DEP\":{\"REGEX\":\"amod|advmod|acl\"},\"OP\":\"?\"},{\"TEXT\":\"ところ\",\"DEP\":{\"REGEX\":\"compound\"}}]\n",
    "Who_pattern3 = [{\"POS\":{\"REGEX\":\"NOUN|PRON|PROPN\"},\"DEP\":{\"REGEX\":\"iobj|obl|nsubj\"}},{\"TEXT\":{\"REGEX\":\"に\"}},{\"TEXT\":{\"REGEX\":\"は\"}}]\n",
    "Who_pattern4 = [{\"POS\":{\"REGEX\":\"NOUN|PRON|PROPN\"},\"DEP\":\"obl\",\"TEXT\":{\"NOT_IN\":[\"度\",\"幾つ\"]}},{\"TEXT\":{\"REGEX\":\"が|は|も\"}}]\n",
    "Who_pattern5 = [{\"TEXT\":\"こと\",\"DEP\":\"compound\"},{\"TEXT\":{\"REGEX\":\"が|は|も\"}}]\n",
    "\n",
    "What_pattern1 = [{\"POS\":{\"REGEX\":\"NOUN\"},\"DEP\":{\"REGEX\":\"obl|obj|iobj\"}},{\"TEXT\":{\"REGEX\":\"に|を\"}}]\n",
    "What_pattern2 = [{\"POS\":{\"REGEX\":\"SYM\"},\"DEP\":{\"REGEX\":\"dep\"}},{\"TEXT\":{\"REGEX\":\"に|を\"}}]\n",
    "\n",
    "\n",
    "Mod_pattern1 = [{\"DEP\":{\"REGEX\":\"amod|advmod|nmod|case|obl|case|acl|acl|aux|det|nsubj|dep|mark|compound|nummod|advcl\"}}]\n",
    "Mod_pattern2 = [{\"POS\":{\"REGEX\":\"NOUN\"},\"DEP\":{\"REGEX\":\"nmod\"}},{\"POS\":\"ADP\"}]\n",
    "Mod_pattern3 = [{\"TEXT\":\"いつ\"}]\n",
    "Mod_pattern4 = [{\"POS\":{\"REGEX\":\"NOUN\"},\"DEP\":\"iobj\"}]\n",
    "\n",
    "\n",
    "def add_label(matcher, doc, id, matches):\n",
    "    l = list(matches[id])\n",
    "    for t in doc[l[1]:l[2]]:\n",
    "        if not t._._5w1h or t._._5w1h == \"Mod\":\n",
    "            t._._5w1h = nlp.vocab.strings[l[0]]\n",
    "\n",
    "\n",
    "def add_right(matcher, doc, id, matches):\n",
    "    l = list(matches[id])\n",
    "    tag =  nlp.vocab.strings[l[0]]\n",
    "    end = l[-1]\n",
    "\n",
    "    if end < len(doc):\n",
    "        while doc[end].head.i == l[1]:\n",
    "            end = end + 1\n",
    "\n",
    "            if end == len(doc):\n",
    "                break\n",
    "            \n",
    "    l[-1] = end\n",
    "\n",
    "    if doc[l[-1]-2:l[-1]].text == \"ので\" and doc[l[-1]].pos_ != \"ADP\":\n",
    "        tag = \"Why\" \n",
    "        \n",
    "    for t in doc[l[1]:l[2]]:\n",
    "        if not t._._5w1h or t._._5w1h == \"Mod\":\n",
    "            t._._5w1h = tag\n",
    "    \n",
    "    matches[id] = tuple(l)\n",
    "\n",
    "def add_right_left(matcher, doc, id, matches):\n",
    "    l = list(matches[id])\n",
    "    tag =  nlp.vocab.strings[l[0]]\n",
    "    end = l[-1]\n",
    "    start = l[1]-1\n",
    "    if end < len(doc):\n",
    "        while doc[end].head.i == l[1]:\n",
    "            end = end + 1\n",
    "\n",
    "            if end == len(doc):\n",
    "                break\n",
    "    if start > -1:\n",
    "        while doc[start].head.i == l[1]:\n",
    "            start = start - 1\n",
    "\n",
    "            if start == -1:\n",
    "                break\n",
    "    start = start +1\n",
    "    l[1] = start\n",
    "    l[-1] = end\n",
    "    \n",
    "    if doc[l[-1]-2:l[-1]].text == \"ので\" and doc[l[-1]].pos_ != \"ADP\":\n",
    "        tag = \"Why\" \n",
    "    \n",
    "    for t in doc[l[1]:l[2]]:\n",
    "        if not t._._5w1h or t._._5w1h == \"Mod\":\n",
    "            t._._5w1h = tag\n",
    "        \n",
    "    matches[id] = tuple(l)\n",
    "\n",
    "\n",
    "matcher.add(\"When\",add_right, When_pattern)\n",
    "matcher.add(\"Where\", add_right, Where_pattern1,Where_pattern2)\n",
    "matcher.add(\"How\", add_right, How_pattern1,How_pattern3,How_pattern3,How_pattern4,How_pattern5,How_pattern6,How_pattern7,How_pattern8,How_pattern9)\n",
    "matcher.add(\"Who\",add_label, Who_pattern1,Who_pattern2,Who_pattern3,Who_pattern4,Who_pattern5)\n",
    "matcher.add(\"What\", add_right, What_pattern1,What_pattern2)\n",
    "matcher.add(\"Why\", add_label, Why_pattern1)\n",
    "matcher.add(\"Mod\", add_right_left,Mod_pattern1,Mod_pattern2,Mod_pattern3,Mod_pattern4,Mod_patttern5)\n",
    "\n",
    "doc = nlp(\"あらすじとレビューを見ていると、殺されると同じ日を何度も繰り返す主人公が犯人を突き止めるホラーサスペンス映画だという印象を受けますが、実際にはホラー要素もサスペンス要素も中途半端な映画です。\")\n",
    "matches = matcher(doc)\n",
    "num = 0\n",
    "start = 0 \n",
    "end = 0\n",
    "\n",
    "tmp_label = None\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    if token._._5w1h == \"Who\" and doc[token.head.i].pos_ == \"VERB\":\n",
    "        t_id = token.head.i\n",
    "        doc[t_id]._._5w1h = \"How\"\n",
    "        t_id = t_id + 1\n",
    "\n",
    "        while doc[t_id].head.i == token.head.i and t_id != len(doc):\n",
    "            doc[t_id]._._5w1h = \"How\"\n",
    "            t_id = t_id + 1\n",
    "\n",
    "            if t_id == len(doc):\n",
    "                break\n",
    "\n",
    "        \n",
    "\n",
    "for token in reversed(doc):\n",
    "    \n",
    "    if token._._5w1h != \"Mod\":\n",
    "        if tmp_label == \"Who\" and token._._5w1h == \"What\":\n",
    "            token._._5w1h = \"Who\"\n",
    "        tmp_label = token._._5w1h\n",
    "    else:\n",
    "        token._._5w1h = tmp_label\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "        #print(token.ent_type)\n",
    "        print(token._._5w1h,token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]._._5w1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日常生活の都合上、'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " doc[0:5].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product_Other'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[5205465455369057791]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
